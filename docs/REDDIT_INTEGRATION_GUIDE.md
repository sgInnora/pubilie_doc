# Reddit é›†æˆæŠ€æœ¯æŒ‡å—

> **é¡¹ç›®**: pubilie_doc Reddit å‘å¸ƒä¸ç›‘æ§
> **ç‰ˆæœ¬**: 1.0.0
> **åˆ›å»ºæ—¥æœŸ**: 2026-01-13
> **çŠ¶æ€**: è°ƒç ”å®Œæˆï¼Œå¾…å®æ–½

---

## ğŸ“‹ ç³»ç»Ÿæ¦‚è§ˆ

### é›†æˆç›®æ ‡

| åŠŸèƒ½ | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| **å†…å®¹å‘å¸ƒ** | è‡ªåŠ¨å‘å¸ƒ AI/å®‰å…¨æ–‡ç« åˆ°ç›¸å…³ subreddit | P0 |
| **çƒ­ç‚¹ç›‘æ§** | ç›‘æ§ç›®æ ‡ subreddit çš„çƒ­é—¨å¸–å­ | P0 |
| **å…³é”®è¯è¿½è¸ª** | è·Ÿè¸ªç‰¹å®šå…³é”®è¯çš„æ–°å¸–å­ | P1 |
| **äº’åŠ¨ç®¡ç†** | ç›‘æ§è¯„è®ºå’Œå›å¤é€šçŸ¥ | P2 |

---

## ğŸ” 1. Reddit API è®¤è¯

### 1.1 åˆ›å»º Reddit App

1. è®¿é—® [Reddit App Preferences](https://old.reddit.com/prefs/apps/)
2. ç‚¹å‡» "create another app..."
3. å¡«å†™ä¿¡æ¯ï¼š
   - **name**: `pubilie-bot`
   - **type**: é€‰æ‹© `script` (ä¸ªäººä½¿ç”¨)
   - **description**: `Content automation for AI/Security articles`
   - **redirect uri**: `http://localhost:8080`
4. ç‚¹å‡» "create app"

### 1.2 è·å–å‡­æ®

åˆ›å»ºåè·å–ï¼š
- **client_id**: åº”ç”¨åç§°ä¸‹æ–¹çš„ 14+ å­—ç¬¦å­—ç¬¦ä¸²
- **client_secret**: `secret` æ—è¾¹çš„ 27+ å­—ç¬¦å­—ç¬¦ä¸²

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ pubilie-bot                             â”‚
â”‚ personal use script                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ client_id: Ab3CdEfGhIjKlM              â”‚  â† è¿™ä¸ª
â”‚ secret: AbCdEfGhIjKlMnOpQrStUvWxYz123  â”‚  â† è¿™ä¸ª
â”‚ redirect uri: http://localhost:8080     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 è®¤è¯æ–¹å¼å¯¹æ¯”

| æ–¹å¼ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ |
|------|----------|--------|
| **Password Flow** | ä¸ªäººè„šæœ¬ï¼Œå•è´¦æˆ· | â­ ç®€å• |
| **Code Flow** | å¤šç”¨æˆ·åº”ç”¨ï¼Œéœ€ç”¨æˆ·æˆæƒ | â­â­â­ å¤æ‚ |
| **Refresh Token** | é•¿æœŸè¿è¡Œçš„è‡ªåŠ¨åŒ–ä»»åŠ¡ | â­â­ ä¸­ç­‰ |

**æ¨è**: ä½¿ç”¨ Password Flow è¿›è¡Œè‡ªåŠ¨åŒ–å‘å¸ƒ

---

## ğŸ“Š 2. API é™åˆ¶

### 2.1 Rate Limits

| è®¤è¯ç±»å‹ | é™åˆ¶ | æ—¶é—´çª—å£ |
|----------|------|----------|
| **OAuth è®¤è¯** | 100 QPM | 10åˆ†é’Ÿå¹³å‡ |
| **æœªè®¤è¯** | 10 QPM | - |

> âš ï¸ **é‡è¦**: è‡ª 2023-07-01 èµ·ï¼Œæœªè®¤è¯è¯·æ±‚ä¼šè¢«é˜»æ–­

### 2.2 å“åº”å¤´ç›‘æ§

```python
# ä»å“åº”å¤´è·å–é™åˆ¶ä¿¡æ¯
X-Ratelimit-Used: 45        # å½“å‰å‘¨æœŸå·²ä½¿ç”¨
X-Ratelimit-Remaining: 55   # å‰©ä½™å¯ç”¨
X-Ratelimit-Reset: 120      # é‡ç½®å€’è®¡æ—¶ï¼ˆç§’ï¼‰
```

### 2.3 å‘å¸ƒé™åˆ¶

| é™åˆ¶ç±»å‹ | è¦æ±‚ |
|----------|------|
| è´¦å·å¹´é¾„ | é€šå¸¸éœ€è¦ >7 å¤© |
| Karma | éƒ¨åˆ† subreddit è¦æ±‚ >10 |
| å‘å¸–é—´éš” | åŒä¸€ subreddit çº¦ 10 åˆ†é’Ÿ |
| å…¨å±€å‘å¸– | çº¦ 1 å¸–/åˆ†é’Ÿ |

---

## ğŸ¯ 3. ç›®æ ‡ Subreddit åˆ—è¡¨

### 3.1 AI/æœºå™¨å­¦ä¹  (å‘å¸ƒ + ç›‘æ§)

| Subreddit | æˆå‘˜æ•° | å†…å®¹ç±»å‹ | è‡ªå‘å¸ƒé€‚åˆåº¦ |
|-----------|--------|----------|--------------|
| r/artificial | 1M+ | æ–°é—»/è®¨è®º | â­â­â­ é«˜ |
| r/MachineLearning | 3M+ | æŠ€æœ¯/è®ºæ–‡ | â­â­ ä¸­ |
| r/ArtificialIntelligence | 1.4M+ | æ–°é—»/äº§å“ | â­â­â­ é«˜ |
| r/LocalLLaMA | 500K+ | å¼€æºLLM | â­â­â­ é«˜ |
| r/ChatGPT | 9M+ | ChatGPTç›¸å…³ | â­â­ ä¸­ |
| r/OpenAI | 2M+ | OpenAIäº§å“ | â­â­ ä¸­ |
| r/learnmachinelearning | 400K+ | æ•™ç¨‹/å…¥é—¨ | â­â­ ä¸­ |
| r/Singularity | 1.8M+ | AIæœªæ¥ | â­â­ ä¸­ |
| r/AGI | 62K+ | AGIè®¨è®º | â­â­ ä¸­ |

### 3.2 ç½‘ç»œå®‰å…¨ (å‘å¸ƒ + ç›‘æ§)

| Subreddit | æˆå‘˜æ•° | å†…å®¹ç±»å‹ | è‡ªå‘å¸ƒé€‚åˆåº¦ |
|-----------|--------|----------|--------------|
| r/cybersecurity | 4M+ | ç»¼åˆå®‰å…¨ | â­â­â­ é«˜ |
| r/netsec | 600K+ | æŠ€æœ¯æ·±åº¦ | â­â­ ä¸­(ä¸¥æ ¼) |
| r/hacking | 3M+ | é»‘å®¢æŠ€æœ¯ | â­â­ ä¸­ |
| r/InfoSecNews | 100K+ | å®‰å…¨æ–°é—» | â­â­â­ é«˜ |
| r/ethicalhacking | 200K+ | æ¸—é€æµ‹è¯• | â­â­ ä¸­ |
| r/cybersecurityai | æ–° | AI+å®‰å…¨ | â­â­â­ é«˜ |
| r/learncybersecurity | 100K+ | å®‰å…¨å…¥é—¨ | â­â­ ä¸­ |

### 3.3 æ¨èå‘å¸ƒç­–ç•¥

```yaml
# å‘å¸ƒä¼˜å…ˆçº§é…ç½®
publishing_strategy:
  tier_1:  # é«˜ä¼˜å…ˆçº§ï¼Œæ¯å‘¨1-2ç¯‡
    - r/artificial
    - r/cybersecurity
    - r/LocalLLaMA
  tier_2:  # ä¸­ä¼˜å…ˆçº§ï¼Œæ¯å‘¨1ç¯‡
    - r/ArtificialIntelligence
    - r/InfoSecNews
    - r/cybersecurityai
  tier_3:  # ä½ä¼˜å…ˆçº§ï¼Œç²¾é€‰å†…å®¹
    - r/MachineLearning
    - r/netsec
```

---

## ğŸ 4. Python å®ç° (PRAW)

### 4.1 å®‰è£…

```bash
pip install praw asyncpraw
```

### 4.2 åŸºç¡€é…ç½®

```python
# config/reddit_config.py
import praw
from praw.models import Subreddit
import os

class RedditClient:
    """Reddit API å®¢æˆ·ç«¯å°è£…"""

    def __init__(self):
        self.reddit = praw.Reddit(
            client_id=os.getenv("REDDIT_CLIENT_ID"),
            client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
            username=os.getenv("REDDIT_USERNAME"),
            password=os.getenv("REDDIT_PASSWORD"),
            user_agent="pubilie-bot/1.0 by u/your_username"
        )

    def verify_auth(self) -> bool:
        """éªŒè¯è®¤è¯çŠ¶æ€"""
        try:
            return self.reddit.user.me() is not None
        except Exception as e:
            print(f"Auth failed: {e}")
            return False
```

### 4.3 å‘å¸ƒåŠŸèƒ½

```python
# publishers/reddit_publisher.py
from typing import Optional, List
import time

class RedditPublisher:
    """Reddit å†…å®¹å‘å¸ƒå™¨"""

    def __init__(self, client: RedditClient):
        self.reddit = client.reddit
        self.last_post_time = {}

    def submit_text_post(
        self,
        subreddit: str,
        title: str,
        content: str,
        flair_id: Optional[str] = None
    ) -> dict:
        """
        å‘å¸ƒæ–‡å­—å¸–å­

        Args:
            subreddit: ç›®æ ‡ subreddit åç§°
            title: å¸–å­æ ‡é¢˜
            content: Markdown æ ¼å¼å†…å®¹
            flair_id: å¯é€‰çš„ flair ID

        Returns:
            åŒ…å«å¸–å­ä¿¡æ¯çš„å­—å…¸
        """
        # æ£€æŸ¥å‘å¸ƒé—´éš”
        self._check_rate_limit(subreddit)

        sub = self.reddit.subreddit(subreddit)

        try:
            submission = sub.submit(
                title=title,
                selftext=content,
                flair_id=flair_id
            )

            self.last_post_time[subreddit] = time.time()

            return {
                "success": True,
                "id": submission.id,
                "url": f"https://reddit.com{submission.permalink}",
                "subreddit": subreddit
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "subreddit": subreddit
            }

    def submit_link_post(
        self,
        subreddit: str,
        title: str,
        url: str,
        flair_id: Optional[str] = None
    ) -> dict:
        """å‘å¸ƒé“¾æ¥å¸–å­"""
        self._check_rate_limit(subreddit)

        sub = self.reddit.subreddit(subreddit)

        try:
            submission = sub.submit(
                title=title,
                url=url,
                flair_id=flair_id
            )

            self.last_post_time[subreddit] = time.time()

            return {
                "success": True,
                "id": submission.id,
                "url": f"https://reddit.com{submission.permalink}",
                "subreddit": subreddit
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    def _check_rate_limit(self, subreddit: str, min_interval: int = 600):
        """æ£€æŸ¥å‘å¸ƒé—´éš”ï¼ˆé»˜è®¤10åˆ†é’Ÿï¼‰"""
        if subreddit in self.last_post_time:
            elapsed = time.time() - self.last_post_time[subreddit]
            if elapsed < min_interval:
                wait_time = min_interval - elapsed
                print(f"Rate limit: waiting {wait_time:.0f}s for r/{subreddit}")
                time.sleep(wait_time)
```

### 4.4 ç›‘æ§åŠŸèƒ½

```python
# collectors/reddit_collector.py
from typing import List, Generator
from datetime import datetime, timedelta

class RedditCollector:
    """Reddit å†…å®¹é‡‡é›†å™¨"""

    def __init__(self, client: RedditClient):
        self.reddit = client.reddit

    def get_hot_posts(
        self,
        subreddit: str,
        limit: int = 25
    ) -> List[dict]:
        """è·å–çƒ­é—¨å¸–å­"""
        sub = self.reddit.subreddit(subreddit)
        posts = []

        for post in sub.hot(limit=limit):
            posts.append(self._parse_post(post))

        return posts

    def get_new_posts(
        self,
        subreddit: str,
        limit: int = 25
    ) -> List[dict]:
        """è·å–æœ€æ–°å¸–å­"""
        sub = self.reddit.subreddit(subreddit)
        posts = []

        for post in sub.new(limit=limit):
            posts.append(self._parse_post(post))

        return posts

    def search_posts(
        self,
        query: str,
        subreddit: str = "all",
        sort: str = "relevance",
        time_filter: str = "week",
        limit: int = 25
    ) -> List[dict]:
        """
        æœç´¢å¸–å­

        Args:
            query: æœç´¢å…³é”®è¯
            subreddit: ç›®æ ‡ subredditï¼Œ"all" ä¸ºå…¨ç«™æœç´¢
            sort: relevance, hot, top, new, comments
            time_filter: hour, day, week, month, year, all
            limit: è¿”å›æ•°é‡
        """
        sub = self.reddit.subreddit(subreddit)
        posts = []

        for post in sub.search(
            query,
            sort=sort,
            time_filter=time_filter,
            limit=limit
        ):
            posts.append(self._parse_post(post))

        return posts

    def stream_new_posts(
        self,
        subreddits: List[str]
    ) -> Generator[dict, None, None]:
        """
        å®æ—¶æµç›‘æ§æ–°å¸–å­

        Args:
            subreddits: subreddit åˆ—è¡¨ï¼Œç”¨ + è¿æ¥

        Yields:
            æ–°å¸–å­å­—å…¸
        """
        sub_str = "+".join(subreddits)
        sub = self.reddit.subreddit(sub_str)

        for post in sub.stream.submissions(skip_existing=True):
            yield self._parse_post(post)

    def _parse_post(self, post) -> dict:
        """è§£æå¸–å­ä¸ºæ ‡å‡†å­—å…¸æ ¼å¼"""
        return {
            "id": post.id,
            "title": post.title,
            "author": str(post.author) if post.author else "[deleted]",
            "subreddit": post.subreddit.display_name,
            "url": f"https://reddit.com{post.permalink}",
            "external_url": post.url if not post.is_self else None,
            "content": post.selftext if post.is_self else None,
            "score": post.score,
            "upvote_ratio": post.upvote_ratio,
            "num_comments": post.num_comments,
            "created_utc": datetime.utcfromtimestamp(post.created_utc).isoformat(),
            "is_self": post.is_self,
            "flair": post.link_flair_text
        }
```

### 4.5 å¼‚æ­¥ç‰ˆæœ¬ (asyncpraw)

```python
# collectors/async_reddit_collector.py
import asyncpraw
import asyncio
from typing import List

class AsyncRedditCollector:
    """å¼‚æ­¥ Reddit é‡‡é›†å™¨"""

    def __init__(self):
        self.reddit = asyncpraw.Reddit(
            client_id=os.getenv("REDDIT_CLIENT_ID"),
            client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
            username=os.getenv("REDDIT_USERNAME"),
            password=os.getenv("REDDIT_PASSWORD"),
            user_agent="pubilie-bot/1.0 by u/your_username"
        )

    async def get_hot_from_multiple(
        self,
        subreddits: List[str],
        limit: int = 10
    ) -> List[dict]:
        """å¹¶å‘è·å–å¤šä¸ª subreddit çš„çƒ­é—¨å¸–å­"""
        tasks = [
            self._get_hot(sub, limit)
            for sub in subreddits
        ]
        results = await asyncio.gather(*tasks)

        # åˆå¹¶å¹¶æŒ‰åˆ†æ•°æ’åº
        all_posts = []
        for posts in results:
            all_posts.extend(posts)

        return sorted(all_posts, key=lambda x: x["score"], reverse=True)

    async def _get_hot(self, subreddit: str, limit: int) -> List[dict]:
        """è·å–å•ä¸ª subreddit çƒ­é—¨å¸–å­"""
        sub = await self.reddit.subreddit(subreddit)
        posts = []

        async for post in sub.hot(limit=limit):
            posts.append({
                "id": post.id,
                "title": post.title,
                "subreddit": subreddit,
                "score": post.score,
                "url": f"https://reddit.com{post.permalink}"
            })

        return posts

    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.reddit.close()
```

---

## ğŸ”„ 5. n8n é›†æˆ

### 5.1 n8n Reddit èŠ‚ç‚¹

n8n æä¾›å®˜æ–¹ Reddit èŠ‚ç‚¹ï¼Œæ”¯æŒï¼š
- **Post**: è·å–å¸–å­ã€æœç´¢
- **Profile**: è·å–ç”¨æˆ·ä¿¡æ¯
- **Subreddit**: è·å– subreddit ä¿¡æ¯

### 5.2 OAuth2 å‡­æ®é…ç½®

åœ¨ n8n ä¸­åˆ›å»º Reddit OAuth2 å‡­æ®ï¼š

```yaml
# n8n å‡­æ®é…ç½®
name: Reddit OAuth2
type: redditOAuth2Api
data:
  clientId: "YOUR_CLIENT_ID"
  clientSecret: "YOUR_CLIENT_SECRET"
  accessToken: ""  # é¦–æ¬¡æˆæƒåè‡ªåŠ¨å¡«å……
  refreshToken: ""
```

### 5.3 å‘å¸ƒå·¥ä½œæµç¤ºä¾‹

```json
{
  "name": "Reddit Auto Publisher",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [{"field": "hours", "hoursInterval": 6}]
        }
      },
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "position": [250, 300]
    },
    {
      "parameters": {
        "operation": "search",
        "query": "SELECT * FROM articles WHERE published_to_reddit = false AND hot_score > 0.7 LIMIT 1"
      },
      "name": "Get Pending Article",
      "type": "n8n-nodes-base.postgres",
      "position": [450, 300]
    },
    {
      "parameters": {
        "resource": "post",
        "operation": "submit",
        "subreddit": "={{ $json.target_subreddit }}",
        "kind": "self",
        "title": "={{ $json.title }}",
        "text": "={{ $json.reddit_content }}"
      },
      "name": "Submit to Reddit",
      "type": "n8n-nodes-base.reddit",
      "position": [650, 300]
    },
    {
      "parameters": {
        "operation": "update",
        "query": "UPDATE articles SET published_to_reddit = true, reddit_post_id = '{{ $json.id }}' WHERE id = '{{ $('Get Pending Article').first().json.id }}'"
      },
      "name": "Update Status",
      "type": "n8n-nodes-base.postgres",
      "position": [850, 300]
    }
  ],
  "connections": {
    "Schedule Trigger": {"main": [[{"node": "Get Pending Article"}]]},
    "Get Pending Article": {"main": [[{"node": "Submit to Reddit"}]]},
    "Submit to Reddit": {"main": [[{"node": "Update Status"}]]}
  }
}
```

### 5.4 ç›‘æ§å·¥ä½œæµç¤ºä¾‹

```json
{
  "name": "Reddit Hot Posts Monitor",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [{"field": "hours", "hoursInterval": 2}]
        }
      },
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "position": [250, 300]
    },
    {
      "parameters": {
        "resource": "post",
        "operation": "getAll",
        "subreddit": "artificial+cybersecurity+LocalLLaMA",
        "returnAll": false,
        "limit": 50,
        "filters": {
          "sort": "hot"
        }
      },
      "name": "Get Hot Posts",
      "type": "n8n-nodes-base.reddit",
      "position": [450, 300]
    },
    {
      "parameters": {
        "jsCode": "// è¿‡æ»¤é«˜åˆ†å¸–å­\nconst posts = $input.all();\nreturn posts.filter(item => item.json.score > 100).map(item => ({\n  json: {\n    title: item.json.title,\n    subreddit: item.json.subreddit,\n    score: item.json.score,\n    url: item.json.url,\n    source: 'reddit'\n  }\n}));"
      },
      "name": "Filter High Score",
      "type": "n8n-nodes-base.code",
      "position": [650, 300]
    },
    {
      "parameters": {
        "operation": "insert",
        "table": "articles",
        "columns": "title, source, url, hot_score, collected_at"
      },
      "name": "Save to DB",
      "type": "n8n-nodes-base.postgres",
      "position": [850, 300]
    }
  ]
}
```

---

## âš ï¸ 6. æœ€ä½³å®è·µä¸é£é™©è§„é¿

### 6.1 é¿å… Shadowban

| é£é™©è¡Œä¸º | å®‰å…¨åšæ³• |
|----------|----------|
| å¿«é€Ÿè¿ç»­å‘å¸– | æ¯ subreddit é—´éš” >10 åˆ†é’Ÿ |
| çº¯è‡ªæˆ‘æ¨å¹¿ | éµå¾ª 10:1 è§„åˆ™ï¼ˆ10æ¡äº’åŠ¨:1æ¡æ¨å¹¿ï¼‰ |
| ç›¸åŒå†…å®¹å¤šå‘ | é’ˆå¯¹ä¸åŒç¤¾åŒºå®šåˆ¶å†…å®¹ |
| æ ‡é¢˜å…š/è¯¯å¯¼ | ä½¿ç”¨å‡†ç¡®æè¿°æ€§æ ‡é¢˜ |
| å¿½ç•¥ç¤¾åŒºè§„åˆ™ | é˜…è¯»å¹¶éµå®ˆæ¯ä¸ª subreddit è§„åˆ™ |

### 6.2 å†…å®¹è´¨é‡è¦æ±‚

```yaml
# å‘å¸ƒå‰æ£€æŸ¥æ¸…å•
pre_publish_checklist:
  - title_length: 60-300 å­—ç¬¦
  - content_length: >500 å­—ç¬¦ï¼ˆself postï¼‰
  - has_value: æä¾›æ–°ä¿¡æ¯æˆ–ç‹¬ç‰¹è§‚ç‚¹
  - not_duplicate: æ£€æŸ¥æ˜¯å¦å·²å‘å¸ƒè¿‡
  - fits_subreddit: åŒ¹é…ç›®æ ‡ç¤¾åŒºä¸»é¢˜
  - proper_flair: é€‰æ‹©æ­£ç¡®çš„ flair
```

### 6.3 æ¨èå‘å¸ƒé¢‘ç‡

| è´¦å·çŠ¶æ€ | å»ºè®®é¢‘ç‡ |
|----------|----------|
| æ–°è´¦å· (<30å¤©) | 1 å¸–/å¤© |
| æˆé•¿æœŸ (30-90å¤©) | 2-3 å¸–/å¤© |
| æˆç†Ÿè´¦å· (>90å¤©) | 5-10 å¸–/å¤© |

### 6.4 Karma ç§¯ç´¯ç­–ç•¥

1. **è¯„è®ºä¼˜å…ˆ**: åœ¨ç›®æ ‡ subreddit æœ‰ä»·å€¼åœ°è¯„è®º
2. **å›ç­”é—®é¢˜**: åœ¨ r/learnmachinelearning ç­‰å¸®åŠ©æ–°æ‰‹
3. **åˆ†äº«èµ„æº**: åˆ†äº«æœ‰ç”¨çš„å·¥å…·å’Œæ•™ç¨‹
4. **å‚ä¸è®¨è®º**: æŠ€æœ¯è®¨è®ºä¸­æä¾›ä¸“ä¸šè§è§£

---

## ğŸ“ 7. ç¯å¢ƒé…ç½®

### 7.1 ç¯å¢ƒå˜é‡

```bash
# .env
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USERNAME=your_username
REDDIT_PASSWORD=your_password
REDDIT_USER_AGENT=pubilie-bot/1.0 by u/your_username
```

### 7.2 praw.ini é…ç½®ï¼ˆæ¨èï¼‰

```ini
# ~/.config/praw.ini æˆ–é¡¹ç›®æ ¹ç›®å½•/praw.ini
[pubilie]
client_id=your_client_id
client_secret=your_client_secret
username=your_username
password=your_password
user_agent=pubilie-bot/1.0 by u/your_username
```

ä½¿ç”¨ï¼š
```python
reddit = praw.Reddit("pubilie")
```

---

## ğŸ“Š 8. æ•°æ®åº“ Schema æ‰©å±•

```sql
-- æ·»åŠ  Reddit ç›¸å…³å­—æ®µåˆ° articles è¡¨
ALTER TABLE articles ADD COLUMN IF NOT EXISTS reddit_post_id VARCHAR(20);
ALTER TABLE articles ADD COLUMN IF NOT EXISTS reddit_subreddit VARCHAR(50);
ALTER TABLE articles ADD COLUMN IF NOT EXISTS published_to_reddit BOOLEAN DEFAULT false;
ALTER TABLE articles ADD COLUMN IF NOT EXISTS reddit_score INT;
ALTER TABLE articles ADD COLUMN IF NOT EXISTS reddit_published_at TIMESTAMPTZ;

-- Reddit ç›‘æ§è®°å½•è¡¨
CREATE TABLE IF NOT EXISTS reddit_monitored_posts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    reddit_id VARCHAR(20) UNIQUE NOT NULL,
    subreddit VARCHAR(50) NOT NULL,
    title TEXT NOT NULL,
    author VARCHAR(50),
    url TEXT,
    score INT,
    num_comments INT,
    created_utc TIMESTAMPTZ,
    collected_at TIMESTAMPTZ DEFAULT NOW(),
    processed BOOLEAN DEFAULT false
);

CREATE INDEX idx_reddit_posts_subreddit ON reddit_monitored_posts(subreddit);
CREATE INDEX idx_reddit_posts_score ON reddit_monitored_posts(score DESC);
```

---

## ğŸ“ å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [PRAW Documentation](https://praw.readthedocs.io/en/stable/)
- [Reddit API Wiki](https://support.reddithelp.com/hc/en-us/articles/16160319875092-Reddit-Data-API-Wiki)
- [n8n Reddit Integration](https://n8n.io/integrations/reddit/)

### æ•™ç¨‹
- [JC Chouinard - Reddit API Guide](https://www.jcchouinard.com/reddit-api/)
- [GeeksforGeeks - PRAW Tutorial](https://www.geeksforgeeks.org/python/python-praw-python-reddit-api-wrapper/)

### Subreddit åˆ—è¡¨
- [Best AI Subreddits 2025](https://usefulai.com/subreddits)
- [Top 50 Cybersecurity Subreddits](https://www.sentinelone.com/blog/top-50-subreddits-for-cybersecurity-and-infosec/)
- [Awesome Cybersecurity Subreddits](https://github.com/d0midigi/awesome-cybersecurity-subreddits)

---

**åˆ›å»ºæ—¶é—´**: 2026-01-13 22:00:00 +0800
**è®¾è®¡è€…**: Claude Opus 4.5
**çŠ¶æ€**: è°ƒç ”å®Œæˆï¼Œå¾…å®æ–½
