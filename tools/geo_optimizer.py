#!/usr/bin/env python3
"""
GEO (Generative Engine Optimization) ä¼˜åŒ–å·¥å…·

ç”¨äºä¼˜åŒ–å†…å®¹ä»¥æå‡åœ¨AIæœç´¢å¼•æ“ä¸­çš„å¼•ç”¨ç‡å’Œå¯è§æ€§ã€‚
æ”¯æŒChatGPTã€Perplexityã€Google AI Overviewsç­‰å¹³å°ã€‚

ç‰ˆæœ¬: 1.0
åˆ›å»ºæ—¶é—´: 2026-01-10
"""

import re
import json
import argparse
from dataclasses import dataclass, field
from typing import Optional
from pathlib import Path


@dataclass
class GEOReport:
    """GEOä¼˜åŒ–æŠ¥å‘Šæ•°æ®ç±»"""
    # ç›´æ¥ç­”æ¡ˆè¯„åˆ†
    has_direct_answer: bool = False
    opening_word_count: int = 0
    direct_answer_score: float = 0.0

    # å¼•ç”¨æ¦‚ç‡è¯„åˆ†
    original_data_count: int = 0
    authority_citations: int = 0
    expert_quotes: int = 0
    case_studies: int = 0
    citation_score: float = 0.0

    # å®ä½“æ ‡è®°
    entities: list = field(default_factory=list)
    suggested_schema: str = "Article"

    # E-E-A-Tè¯„åˆ†
    has_author_credentials: bool = False
    has_experience_description: bool = False
    has_traceable_sources: bool = False
    has_update_date: bool = False
    eeat_score: float = 0.0

    # æ€»åˆ†
    total_score: float = 0.0
    recommendations: list = field(default_factory=list)


class GEOOptimizer:
    """GEOä¼˜åŒ–å™¨ä¸»ç±»"""

    # æƒå¨æ¥æºå…³é”®è¯
    AUTHORITY_SOURCES = [
        'MITRE', 'NIST', 'Gartner', 'Forrester', 'IDC',
        'IEEE', 'ACM', 'OWASP', 'CISA', 'FBI', 'NSA',
        'Microsoft', 'Google', 'Amazon', 'CrowdStrike',
        'Mandiant', 'Recorded Future', 'Palo Alto'
    ]

    # æ•°æ®æŒ‡æ ‡å…³é”®è¯
    DATA_INDICATORS = [
        r'\d+%', r'\d+\s*(million|billion|ä¸‡|äº¿)',
        r'increased by', r'decreased by', r'growth of',
        r'ç»Ÿè®¡', r'æ•°æ®æ˜¾ç¤º', r'ç ”ç©¶è¡¨æ˜', r'è°ƒæŸ¥å‘ç°'
    ]

    # ä¸“å®¶å¼•ç”¨æ¨¡å¼
    EXPERT_PATTERNS = [
        r'according to\s+\w+',
        r'said\s+\w+',
        r'\w+\s+è¡¨ç¤º',
        r'\w+\s+æŒ‡å‡º',
        r'ç ”ç©¶å‘˜\s+\w+',
        r'ä¸“å®¶\s+\w+'
    ]

    # æ¡ˆä¾‹ç ”ç©¶å…³é”®è¯
    CASE_STUDY_KEYWORDS = [
        'case study', 'real-world', 'example',
        'æ¡ˆä¾‹', 'å®ä¾‹', 'å®é™…åº”ç”¨', 'çœŸå®åœºæ™¯'
    ]

    # å®‰å…¨å®ä½“æ¨¡å¼
    SECURITY_ENTITIES = {
        'cve': r'CVE-\d{4}-\d+',
        'apt': r'APT\d+|APT-\d+',
        'attack_id': r'T\d{4}(?:\.\d{3})?',
        'malware': r'[A-Z][a-z]+(?:Bot|Trojan|Ransomware|Worm|Backdoor)',
    }

    # è¿‚å›å¼€å¤´é»‘åå•
    INDIRECT_OPENINGS = [
        'åœ¨æœ¬æ–‡ä¸­', 'æœ¬æ–‡å°†', 'è®©æˆ‘ä»¬æ¢è®¨', 'è®©æˆ‘ä»¬æ·±å…¥',
        'In this article', 'This article will', 'Let us explore',
        'éšç€.*çš„å‘å±•', 'è¿‘å¹´æ¥', 'ä¼—æ‰€å‘¨çŸ¥'
    ]

    def __init__(self):
        pass

    def analyze(self, content: str) -> GEOReport:
        """
        åˆ†æå†…å®¹çš„GEOä¼˜åŒ–ç¨‹åº¦

        Args:
            content: Markdownæ ¼å¼çš„æ–‡ç« å†…å®¹

        Returns:
            GEOReport: GEOä¼˜åŒ–æŠ¥å‘Š
        """
        report = GEOReport()

        # 1. åˆ†æç›´æ¥ç­”æ¡ˆ
        report = self._analyze_direct_answer(content, report)

        # 2. åˆ†æå¼•ç”¨æ¦‚ç‡
        report = self._analyze_citation_probability(content, report)

        # 3. æå–å®ä½“
        report = self._extract_entities(content, report)

        # 4. åˆ†æE-E-A-T
        report = self._analyze_eeat(content, report)

        # 5. è®¡ç®—æ€»åˆ†
        report.total_score = self._calculate_total_score(report)

        # 6. ç”Ÿæˆå»ºè®®
        report.recommendations = self._generate_recommendations(report)

        return report

    def _analyze_direct_answer(self, content: str, report: GEOReport) -> GEOReport:
        """åˆ†æç›´æ¥ç­”æ¡ˆè¯„åˆ†"""
        lines = content.split('\n')

        # è·³è¿‡frontmatterå’Œæ ‡é¢˜ï¼Œæ‰¾åˆ°æ­£æ–‡å¼€å¤´
        in_frontmatter = False
        first_paragraph = ""

        for line in lines:
            if line.strip() == '---':
                in_frontmatter = not in_frontmatter
                continue
            if in_frontmatter:
                continue
            if line.startswith('#'):
                continue
            if line.strip():
                first_paragraph = line.strip()
                break

        # è®¡ç®—å¼€å¤´å­—æ•°
        report.opening_word_count = len(first_paragraph)

        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿‚å›å¼€å¤´
        is_indirect = any(
            re.search(pattern, first_paragraph, re.IGNORECASE)
            for pattern in self.INDIRECT_OPENINGS
        )

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥å®šä¹‰/å›ç­”æ ¼å¼
        direct_patterns = [
            r'^[\w\u4e00-\u9fa5]+\s*(æ˜¯|is|are|refers to|means)',
            r'^(What|How|Why|When|Where).*\?.*:',
            r'^\*\*[\w\u4e00-\u9fa5]+\*\*\s*[:ï¼š]',
        ]
        is_direct = any(
            re.search(pattern, first_paragraph, re.IGNORECASE)
            for pattern in direct_patterns
        )

        report.has_direct_answer = is_direct and not is_indirect

        # è¯„åˆ†: ç›´æ¥å›ç­”10åˆ†ï¼Œå¼€å¤´ç®€æ´(<50å­—)+5åˆ†
        score = 0
        if report.has_direct_answer:
            score += 10
        if report.opening_word_count < 50:
            score += 3
        elif report.opening_word_count < 100:
            score += 1

        report.direct_answer_score = min(score, 10)

        return report

    def _analyze_citation_probability(self, content: str, report: GEOReport) -> GEOReport:
        """åˆ†æå¼•ç”¨æ¦‚ç‡è¯„åˆ†"""
        # ç»Ÿè®¡åŸåˆ›æ•°æ®
        for pattern in self.DATA_INDICATORS:
            report.original_data_count += len(re.findall(pattern, content, re.IGNORECASE))

        # ç»Ÿè®¡æƒå¨å¼•ç”¨
        for source in self.AUTHORITY_SOURCES:
            if source.lower() in content.lower():
                report.authority_citations += 1

        # ç»Ÿè®¡ä¸“å®¶å¼•ç”¨
        for pattern in self.EXPERT_PATTERNS:
            report.expert_quotes += len(re.findall(pattern, content, re.IGNORECASE))

        # ç»Ÿè®¡æ¡ˆä¾‹ç ”ç©¶
        for keyword in self.CASE_STUDY_KEYWORDS:
            if keyword.lower() in content.lower():
                report.case_studies += 1

        # è®¡ç®—å¼•ç”¨æ¦‚ç‡åˆ†æ•° (0-10)
        score = 0
        score += min(report.original_data_count * 1.5, 4)  # æœ€å¤š4åˆ†
        score += min(report.authority_citations * 1, 3)     # æœ€å¤š3åˆ†
        score += min(report.expert_quotes * 0.5, 2)         # æœ€å¤š2åˆ†
        score += min(report.case_studies * 1, 1)            # æœ€å¤š1åˆ†

        report.citation_score = min(score, 10)

        return report

    def _extract_entities(self, content: str, report: GEOReport) -> GEOReport:
        """æå–å®‰å…¨ç›¸å…³å®ä½“"""
        entities = []

        for entity_type, pattern in self.SECURITY_ENTITIES.items():
            matches = re.findall(pattern, content)
            for match in matches:
                entities.append({
                    'type': entity_type,
                    'value': match
                })

        report.entities = entities

        # æ ¹æ®å®ä½“ç±»å‹å»ºè®®Schema
        if any(e['type'] == 'cve' for e in entities):
            report.suggested_schema = "TechArticle"
        elif any(e['type'] == 'apt' for e in entities):
            report.suggested_schema = "Article"  # å¨èƒæƒ…æŠ¥æ–‡ç« 
        else:
            report.suggested_schema = "BlogPosting"

        return report

    def _analyze_eeat(self, content: str, report: GEOReport) -> GEOReport:
        """åˆ†æE-E-A-Tè¯„åˆ†"""
        content_lower = content.lower()

        # æ£€æŸ¥ä½œè€…èµ„è´¨
        author_patterns = [
            r'author:', r'ä½œè€…:', r'by\s+\w+',
            r'security\s+researcher', r'å®‰å…¨ç ”ç©¶å‘˜',
            r'research\s+team', r'ç ”ç©¶å›¢é˜Ÿ'
        ]
        report.has_author_credentials = any(
            re.search(p, content, re.IGNORECASE) for p in author_patterns
        )

        # æ£€æŸ¥å®é™…ç»éªŒæè¿°
        experience_patterns = [
            r'æˆ‘(ä»¬)?å®é™…', r'æˆ‘(ä»¬)?æµ‹è¯•', r'æˆ‘(ä»¬)?å‘ç°',
            r'in our experience', r'we tested', r'we found',
            r'å®æˆ˜', r'å®æ“', r'hands-on'
        ]
        report.has_experience_description = any(
            re.search(p, content, re.IGNORECASE) for p in experience_patterns
        )

        # æ£€æŸ¥æ¥æºå¯è¿½æº¯æ€§
        source_patterns = [
            r'https?://', r'æ¥æº:', r'source:', r'reference:',
            r'\[.*\]\(http', r'å¼•ç”¨è‡ª', r'according to'
        ]
        report.has_traceable_sources = any(
            re.search(p, content, re.IGNORECASE) for p in source_patterns
        )

        # æ£€æŸ¥æ›´æ–°æ—¥æœŸ
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}', r'\d{4}å¹´\d{1,2}æœˆ',
            r'last\s+updated', r'æœ€åæ›´æ–°', r'æ›´æ–°æ—¶é—´'
        ]
        report.has_update_date = any(
            re.search(p, content, re.IGNORECASE) for p in date_patterns
        )

        # è®¡ç®—E-E-A-Tåˆ†æ•°
        score = 0
        if report.has_author_credentials:
            score += 2.5
        if report.has_experience_description:
            score += 2.5
        if report.has_traceable_sources:
            score += 2.5
        if report.has_update_date:
            score += 2.5

        report.eeat_score = score

        return report

    def _calculate_total_score(self, report: GEOReport) -> float:
        """è®¡ç®—æ€»åˆ†"""
        # æƒé‡: ç›´æ¥ç­”æ¡ˆ25%, å¼•ç”¨æ¦‚ç‡40%, E-E-A-T35%
        total = (
            report.direct_answer_score * 0.25 +
            report.citation_score * 0.40 +
            report.eeat_score * 0.35
        )
        return round(total, 2)

    def _generate_recommendations(self, report: GEOReport) -> list:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # ç›´æ¥ç­”æ¡ˆå»ºè®®
        if not report.has_direct_answer:
            recommendations.append(
                "å¼€å¤´åº”ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜ï¼Œé¿å…è¿‚å›è¡¨è¾¾ã€‚"
                "ä½¿ç”¨'Xæ˜¯...'æˆ–'X refers to...'æ ¼å¼å¼€å§‹ã€‚"
            )

        # å¼•ç”¨æ¦‚ç‡å»ºè®®
        if report.original_data_count < 3:
            recommendations.append(
                f"å½“å‰ä»…æœ‰{report.original_data_count}ä¸ªæ•°æ®ç‚¹ï¼Œ"
                "å»ºè®®æ·»åŠ è‡³å°‘3ä¸ªåŸåˆ›ç»Ÿè®¡/æ•°æ®ä»¥æå‡å¼•ç”¨æ¦‚ç‡ã€‚"
            )

        if report.authority_citations < 2:
            recommendations.append(
                "å»ºè®®å¼•ç”¨æ›´å¤šæƒå¨æ¥æº(MITRE, NIST, Gartnerç­‰)ä»¥å¢å¼ºå¯ä¿¡åº¦ã€‚"
            )

        # E-E-A-Tå»ºè®®
        if not report.has_author_credentials:
            recommendations.append(
                "å»ºè®®æ·»åŠ ä½œè€…/å›¢é˜Ÿèµ„è´¨è¯´æ˜ä»¥æå‡æƒå¨æ€§ã€‚"
            )

        if not report.has_experience_description:
            recommendations.append(
                "å»ºè®®æ·»åŠ å®é™…æ“ä½œç»éªŒæè¿°ï¼Œ"
                "å¦‚'æˆ‘ä»¬å®é™…æµ‹è¯•å‘ç°...'ä»¥å¢å¼ºExperienceç»´åº¦ã€‚"
            )

        if not report.has_traceable_sources:
            recommendations.append(
                "å»ºè®®æ·»åŠ å¯è¿½æº¯çš„å¼•ç”¨é“¾æ¥ï¼Œæå‡å†…å®¹å¯ä¿¡åº¦ã€‚"
            )

        return recommendations

    def generate_key_takeaways(self, content: str, count: int = 3) -> list:
        """
        ç”Ÿæˆæ–‡ç« å…³é”®è¦ç‚¹ï¼ˆç”¨äºGEOä¼˜åŒ–ï¼‰

        Args:
            content: æ–‡ç« å†…å®¹
            count: è¦ç‚¹æ•°é‡

        Returns:
            å…³é”®è¦ç‚¹åˆ—è¡¨
        """
        # æå–æ ‡é¢˜ä½œä¸ºè¦ç‚¹åŸºç¡€
        headers = re.findall(r'^##\s+(.+)$', content, re.MULTILINE)

        takeaways = []
        for header in headers[:count]:
            # æ¸…ç†æ ‡é¢˜
            clean_header = re.sub(r'[\d\.\)]+\s*', '', header).strip()
            if clean_header and len(clean_header) > 5:
                takeaways.append(clean_header)

        return takeaways[:count]

    def generate_faq_pairs(self, content: str) -> list:
        """
        ä»å†…å®¹ä¸­æå–FAQå¯¹ï¼ˆç”¨äºFAQPage Schemaï¼‰

        Args:
            content: æ–‡ç« å†…å®¹

        Returns:
            FAQå¯¹åˆ—è¡¨ [{'question': str, 'answer': str}]
        """
        faqs = []

        # æ¨¡å¼1: æ ‡é¢˜æ˜¯é—®é¢˜æ ¼å¼
        question_headers = re.findall(
            r'^##\s*(.+\?)\s*\n((?:(?!^#).+\n?)+)',
            content, re.MULTILINE
        )
        for q, a in question_headers:
            faqs.append({
                'question': q.strip(),
                'answer': a.strip()[:500]  # é™åˆ¶ç­”æ¡ˆé•¿åº¦
            })

        # æ¨¡å¼2: Q: A: æ ¼å¼
        qa_pattern = re.findall(
            r'(?:Q|é—®)[:ï¼š]\s*(.+?)\n(?:A|ç­”)[:ï¼š]\s*(.+?)(?=\n(?:Q|é—®)[:ï¼š]|\n#|\Z)',
            content, re.DOTALL
        )
        for q, a in qa_pattern:
            faqs.append({
                'question': q.strip(),
                'answer': a.strip()[:500]
            })

        return faqs

    def format_report(self, report: GEOReport) -> str:
        """æ ¼å¼åŒ–GEOæŠ¥å‘Šä¸ºMarkdown"""
        output = f"""## ğŸ¯ GEOä¼˜åŒ–æŠ¥å‘Š

### ç›´æ¥ç­”æ¡ˆè¯„åˆ†: {report.direct_answer_score}/10
- å¼€å¤´æ˜¯å¦ç›´æ¥å›ç­”: {'âœ…' if report.has_direct_answer else 'âŒ'}
- å¼€å¤´å­—æ•°: {report.opening_word_count}å­—

### å¼•ç”¨æ¦‚ç‡è¯„åˆ†: {report.citation_score}/10
- åŸåˆ›æ•°æ®æ•°é‡: {report.original_data_count}ä¸ª
- æƒå¨å¼•ç”¨æ•°é‡: {report.authority_citations}ä¸ª
- ä¸“å®¶è§‚ç‚¹æ•°é‡: {report.expert_quotes}ä¸ª
- æ¡ˆä¾‹ç ”ç©¶æ•°é‡: {report.case_studies}ä¸ª

### å®ä½“è¯†åˆ«
- è¯†åˆ«å®ä½“æ•°: {len(report.entities)}ä¸ª
- å®ä½“åˆ—è¡¨: {', '.join([e['value'] for e in report.entities[:10]])}
- å»ºè®®Schema: {report.suggested_schema}

### E-E-A-Tè¯„åˆ†: {report.eeat_score}/10
- ä½œè€…èµ„è´¨å±•ç¤º: {'âœ…' if report.has_author_credentials else 'âŒ'}
- å®é™…ç»éªŒæè¿°: {'âœ…' if report.has_experience_description else 'âŒ'}
- æ¥æºå¯è¿½æº¯æ€§: {'âœ…' if report.has_traceable_sources else 'âŒ'}
- æ›´æ–°æ—¥æœŸæ ‡æ³¨: {'âœ…' if report.has_update_date else 'âŒ'}

### ğŸ“Š æ€»åˆ†: {report.total_score}/10

### ğŸ’¡ ä¼˜åŒ–å»ºè®®
"""
        for i, rec in enumerate(report.recommendations, 1):
            output += f"{i}. {rec}\n"

        if not report.recommendations:
            output += "æš‚æ— å»ºè®®ï¼Œå†…å®¹GEOä¼˜åŒ–è‰¯å¥½ï¼\n"

        return output


def main():
    parser = argparse.ArgumentParser(description='GEOä¼˜åŒ–å·¥å…·')
    parser.add_argument('file', help='è¦åˆ†æçš„Markdownæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„(å¯é€‰)')
    parser.add_argument('--json', action='store_true', help='è¾“å‡ºJSONæ ¼å¼')

    args = parser.parse_args()

    # è¯»å–æ–‡ä»¶
    file_path = Path(args.file)
    if not file_path.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.file}")
        return 1

    content = file_path.read_text(encoding='utf-8')

    # åˆ†æ
    optimizer = GEOOptimizer()
    report = optimizer.analyze(content)

    # è¾“å‡º
    if args.json:
        output = json.dumps({
            'direct_answer_score': report.direct_answer_score,
            'citation_score': report.citation_score,
            'eeat_score': report.eeat_score,
            'total_score': report.total_score,
            'entities': report.entities,
            'recommendations': report.recommendations
        }, ensure_ascii=False, indent=2)
    else:
        output = optimizer.format_report(report)

    if args.output:
        Path(args.output).write_text(output, encoding='utf-8')
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print(output)

    return 0


if __name__ == '__main__':
    exit(main())
