#!/usr/bin/env python3
"""
deep_ai_detector.py - æ·±åº¦å­¦ä¹ AIæ£€æµ‹æ¨¡å— v2.0

ç”¨é€”:
    è¡¥å……check_ai_traces.shçš„è§„åˆ™æ£€æµ‹ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ APIè¿›è¡Œç¬¬ä¸‰å±‚éªŒè¯
    æ”¯æŒå¤šæ£€æµ‹å™¨èšåˆè¯„åˆ†ï¼Œæå‡æ£€æµ‹å‡†ç¡®æ€§

èƒŒæ™¯:
    Pangram Labsç ”ç©¶è¡¨æ˜Perplexity/Burstinessè§„åˆ™æ£€æµ‹å‡é˜³æ€§ç‡1-50%
    æ·±åº¦å­¦ä¹ æ¨¡å‹å¯æä¾›æ›´å‡†ç¡®çš„æ£€æµ‹ç»“æœ
    å¤šæ£€æµ‹å™¨èšåˆå¯è¿›ä¸€æ­¥é™ä½è¯¯åˆ¤ç‡

æ”¯æŒçš„æ£€æµ‹æœåŠ¡:
    - GPTZero API (æ¨èï¼Œå­¦æœ¯ç”¨é€”å…è´¹é¢åº¦)
    - Originality.ai API (å•†ä¸šä»˜è´¹ï¼Œç²¾åº¦æœ€é«˜)
    - ZeroGPT API (å…è´¹é¢åº¦ï¼Œå¿«é€Ÿæ£€æµ‹)
    - Copyleaks API (ä¼ä¸šçº§ï¼Œæ”¯æŒå¤šè¯­è¨€)
    - Winston AI (å¤‡é€‰)
    - è‡ªå®šä¹‰LLM (Qwen/Claude/GPT)

æ–°å¢åŠŸèƒ½ (v2.0):
    - å¤šæ£€æµ‹å™¨èšåˆè¯„åˆ†
    - ZeroGPT/Copyleaks APIæ”¯æŒ
    - åŠ æƒå¹³å‡ç½®ä¿¡åº¦è®¡ç®—
    - æ£€æµ‹å™¨ä¸€è‡´æ€§åˆ†æ

ç”¨æ³•:
    # å•æ–‡ä»¶æ£€æµ‹
    python deep_ai_detector.py --file article.md

    # å¤šæ£€æµ‹å™¨èšåˆæ¨¡å¼
    python deep_ai_detector.py --file article.md --aggregate

    # JSONè¾“å‡º (ä¾›n8nè°ƒç”¨)
    python deep_ai_detector.py --file article.md --json

    # æ‰¹é‡æ£€æµ‹
    python deep_ai_detector.py --dir 2026_01/ --pattern "*.md"

ä½œè€…: Innora Security Research Team
ç‰ˆæœ¬: 2.0 | æ—¥æœŸ: 2026-01-10
"""

import os
import sys
import json
import argparse
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

# å¯é€‰ä¾èµ–
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

try:
    from dotenv import load_dotenv
    load_dotenv(Path.home() / ".env")
    HAS_DOTENV = True
except ImportError:
    HAS_DOTENV = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class DetectorConfig:
    """æ£€æµ‹å™¨é…ç½®"""
    # æœåŠ¡é€‰æ‹©: gptzero, originality, winston, custom_llm
    service: str = "gptzero"

    # GPTZeroé…ç½®
    gptzero_api_key: str = ""
    gptzero_endpoint: str = "https://api.gptzero.me/v2/predict/text"

    # Originality.aié…ç½®
    originality_api_key: str = ""
    originality_endpoint: str = "https://api.originality.ai/api/v1/scan/ai"

    # Winston AIé…ç½®
    winston_api_key: str = ""
    winston_endpoint: str = "https://api.gowinston.ai/v2/detect"

    # ZeroGPTé…ç½® (v2.0æ–°å¢)
    zerogpt_api_key: str = ""
    zerogpt_endpoint: str = "https://api.zerogpt.com/api/detect/detectText"

    # Copyleaksé…ç½® (v2.0æ–°å¢)
    copyleaks_api_key: str = ""
    copyleaks_email: str = ""
    copyleaks_endpoint: str = "https://api.copyleaks.com/v2/writer-detector"

    # è‡ªå®šä¹‰LLMé…ç½® (ä½¿ç”¨vLLMæˆ–OpenAIå…¼å®¹API)
    custom_llm_endpoint: str = ""
    custom_llm_api_key: str = ""
    custom_llm_model: str = "Qwen/Qwen2.5-72B-Instruct"

    # æ£€æµ‹é˜ˆå€¼
    ai_threshold: float = 0.7  # >0.7 åˆ¤å®šä¸ºAIç”Ÿæˆ
    human_threshold: float = 0.3  # <0.3 åˆ¤å®šä¸ºäººç±»å†™ä½œ

    # ç¼“å­˜
    cache_dir: str = ".ai_detection_cache"
    cache_ttl_hours: int = 24

    @classmethod
    def from_env(cls) -> "DetectorConfig":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            service=os.getenv("AI_DETECTOR_SERVICE", "gptzero"),
            gptzero_api_key=os.getenv("GPTZERO_API_KEY", ""),
            originality_api_key=os.getenv("ORIGINALITY_API_KEY", ""),
            winston_api_key=os.getenv("WINSTON_API_KEY", ""),
            zerogpt_api_key=os.getenv("ZEROGPT_API_KEY", ""),
            copyleaks_api_key=os.getenv("COPYLEAKS_API_KEY", ""),
            copyleaks_email=os.getenv("COPYLEAKS_EMAIL", ""),
            custom_llm_endpoint=os.getenv("VLLM_HOST", os.getenv("CUSTOM_LLM_ENDPOINT", "")),
            custom_llm_api_key=os.getenv("VLLM_API_KEY", os.getenv("CUSTOM_LLM_API_KEY", "")),
            custom_llm_model=os.getenv("CUSTOM_LLM_MODEL", "Qwen/Qwen2.5-72B-Instruct"),
            ai_threshold=float(os.getenv("AI_DETECTOR_THRESHOLD", "0.7")),
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ£€æµ‹ç»“æœ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœ"""
    file_path: str
    service: str
    timestamp: str

    # æ ¸å¿ƒåˆ†æ•° (0-1, 1=å®Œå…¨AIç”Ÿæˆ)
    ai_probability: float
    human_probability: float
    mixed_probability: float

    # åˆ¤å®šç»“æœ
    classification: str  # "ai", "human", "mixed", "uncertain"
    confidence: float

    # è¯¦ç»†åˆ†æ (æœåŠ¡ç›¸å…³)
    sentence_scores: Optional[List[float]] = None
    paragraph_scores: Optional[List[float]] = None
    highlighted_ai_sentences: Optional[List[str]] = None

    # å…ƒæ•°æ®
    text_length: int = 0
    language: str = "auto"
    error: Optional[str] = None

    def to_dict(self) -> Dict:
        return asdict(self)

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)


@dataclass
class AggregateResult:
    """å¤šæ£€æµ‹å™¨èšåˆç»“æœ (v2.0æ–°å¢)"""
    file_path: str
    timestamp: str

    # èšåˆåˆ†æ•°
    weighted_ai_probability: float
    weighted_human_probability: float
    consensus_classification: str  # å¤šæ•°æŠ•ç¥¨ç»“æœ
    consensus_confidence: float
    agreement_ratio: float  # æ£€æµ‹å™¨ä¸€è‡´æ€§ (0-1)

    # å„æ£€æµ‹å™¨ç»“æœ
    individual_results: List[Dict] = field(default_factory=list)
    successful_detectors: int = 0
    failed_detectors: int = 0

    # æ¨è
    recommendation: str = ""
    needs_review: bool = False  # ç»“æœåˆ†æ­§æ—¶éœ€è¦äººå·¥å®¡æ ¸

    def to_dict(self) -> Dict:
        return asdict(self)

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ£€æµ‹æœåŠ¡é€‚é…å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BaseDetector:
    """æ£€æµ‹å™¨åŸºç±»"""

    def __init__(self, config: DetectorConfig):
        self.config = config

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        raise NotImplementedError

    def _create_error_result(self, file_path: str, error: str) -> DetectionResult:
        return DetectionResult(
            file_path=file_path,
            service=self.__class__.__name__,
            timestamp=datetime.now().isoformat(),
            ai_probability=0.5,
            human_probability=0.5,
            mixed_probability=0.0,
            classification="uncertain",
            confidence=0.0,
            error=error
        )


class GPTZeroDetector(BaseDetector):
    """GPTZero APIæ£€æµ‹å™¨"""

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        if not self.config.gptzero_api_key:
            return self._create_error_result(file_path, "GPTZERO_API_KEY not configured")

        if not HAS_REQUESTS:
            return self._create_error_result(file_path, "requests library not installed")

        try:
            response = requests.post(
                self.config.gptzero_endpoint,
                headers={
                    "X-Api-Key": self.config.gptzero_api_key,
                    "Content-Type": "application/json"
                },
                json={"document": text[:50000]},  # GPTZeroé™åˆ¶50kå­—ç¬¦
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            # è§£æGPTZeroå“åº”
            doc = data.get("documents", [{}])[0]
            ai_prob = doc.get("completely_generated_prob", 0.5)
            mixed_prob = doc.get("average_generated_prob", 0.5)

            # åˆ†ç±»é€»è¾‘
            if ai_prob > self.config.ai_threshold:
                classification = "ai"
                confidence = ai_prob
            elif ai_prob < self.config.human_threshold:
                classification = "human"
                confidence = 1 - ai_prob
            else:
                classification = "mixed"
                confidence = 1 - abs(ai_prob - 0.5) * 2

            return DetectionResult(
                file_path=file_path,
                service="GPTZero",
                timestamp=datetime.now().isoformat(),
                ai_probability=ai_prob,
                human_probability=1 - ai_prob,
                mixed_probability=mixed_prob,
                classification=classification,
                confidence=confidence,
                sentence_scores=doc.get("sentences", []),
                text_length=len(text)
            )

        except Exception as e:
            return self._create_error_result(file_path, str(e))


class OriginalityDetector(BaseDetector):
    """Originality.ai APIæ£€æµ‹å™¨"""

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        if not self.config.originality_api_key:
            return self._create_error_result(file_path, "ORIGINALITY_API_KEY not configured")

        if not HAS_REQUESTS:
            return self._create_error_result(file_path, "requests library not installed")

        try:
            response = requests.post(
                self.config.originality_endpoint,
                headers={
                    "X-OAI-API-KEY": self.config.originality_api_key,
                    "Content-Type": "application/json"
                },
                json={"content": text[:25000]},  # Originalityé™åˆ¶25kå­—ç¬¦
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            ai_score = data.get("score", {}).get("ai", 0.5)
            original_score = data.get("score", {}).get("original", 0.5)

            if ai_score > self.config.ai_threshold:
                classification = "ai"
            elif ai_score < self.config.human_threshold:
                classification = "human"
            else:
                classification = "mixed"

            return DetectionResult(
                file_path=file_path,
                service="Originality.ai",
                timestamp=datetime.now().isoformat(),
                ai_probability=ai_score,
                human_probability=original_score,
                mixed_probability=0.0,
                classification=classification,
                confidence=max(ai_score, original_score),
                text_length=len(text)
            )

        except Exception as e:
            return self._create_error_result(file_path, str(e))


class ZeroGPTDetector(BaseDetector):
    """ZeroGPT APIæ£€æµ‹å™¨ (v2.0æ–°å¢)"""

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        if not self.config.zerogpt_api_key:
            return self._create_error_result(file_path, "ZEROGPT_API_KEY not configured")

        if not HAS_REQUESTS:
            return self._create_error_result(file_path, "requests library not installed")

        try:
            response = requests.post(
                self.config.zerogpt_endpoint,
                headers={
                    "ApiKey": self.config.zerogpt_api_key,
                    "Content-Type": "application/json"
                },
                json={"input_text": text[:50000]},
                timeout=30
            )
            response.raise_for_status()
            data = response.json()

            # è§£æZeroGPTå“åº”
            # ZeroGPTè¿”å›fake_percentage (0-100)
            fake_pct = data.get("data", {}).get("fakePercentage", 50)
            ai_prob = fake_pct / 100.0

            if ai_prob > self.config.ai_threshold:
                classification = "ai"
            elif ai_prob < self.config.human_threshold:
                classification = "human"
            else:
                classification = "mixed"

            return DetectionResult(
                file_path=file_path,
                service="ZeroGPT",
                timestamp=datetime.now().isoformat(),
                ai_probability=ai_prob,
                human_probability=1 - ai_prob,
                mixed_probability=0.0,
                classification=classification,
                confidence=max(ai_prob, 1 - ai_prob),
                highlighted_ai_sentences=data.get("data", {}).get("h", []),
                text_length=len(text)
            )

        except Exception as e:
            return self._create_error_result(file_path, str(e))


class CopyleaksDetector(BaseDetector):
    """Copyleaks AIæ£€æµ‹å™¨ (v2.0æ–°å¢ï¼Œä¼ä¸šçº§)"""

    def __init__(self, config: DetectorConfig):
        super().__init__(config)
        self._access_token = None

    def _get_access_token(self) -> Optional[str]:
        """è·å–Copyleaksè®¿é—®ä»¤ç‰Œ"""
        if self._access_token:
            return self._access_token

        if not self.config.copyleaks_email or not self.config.copyleaks_api_key:
            return None

        try:
            response = requests.post(
                "https://id.copyleaks.com/v3/account/login/api",
                json={
                    "email": self.config.copyleaks_email,
                    "key": self.config.copyleaks_api_key
                },
                timeout=30
            )
            response.raise_for_status()
            data = response.json()
            self._access_token = data.get("access_token")
            return self._access_token
        except Exception:
            return None

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        if not self.config.copyleaks_api_key:
            return self._create_error_result(file_path, "COPYLEAKS_API_KEY not configured")

        if not HAS_REQUESTS:
            return self._create_error_result(file_path, "requests library not installed")

        token = self._get_access_token()
        if not token:
            return self._create_error_result(file_path, "Failed to authenticate with Copyleaks")

        try:
            response = requests.post(
                self.config.copyleaks_endpoint,
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json"
                },
                json={"text": text[:25000]},
                timeout=60
            )
            response.raise_for_status()
            data = response.json()

            # è§£æCopyleakså“åº”
            summary = data.get("summary", {})
            ai_prob = summary.get("ai", 0.5)
            human_prob = summary.get("human", 0.5)

            if ai_prob > self.config.ai_threshold:
                classification = "ai"
            elif ai_prob < self.config.human_threshold:
                classification = "human"
            else:
                classification = "mixed"

            return DetectionResult(
                file_path=file_path,
                service="Copyleaks",
                timestamp=datetime.now().isoformat(),
                ai_probability=ai_prob,
                human_probability=human_prob,
                mixed_probability=1 - ai_prob - human_prob if ai_prob + human_prob < 1 else 0,
                classification=classification,
                confidence=max(ai_prob, human_prob),
                text_length=len(text)
            )

        except Exception as e:
            return self._create_error_result(file_path, str(e))


class CustomLLMDetector(BaseDetector):
    """è‡ªå®šä¹‰LLMæ£€æµ‹å™¨ (Qwen/Claude/GPTå…¼å®¹)"""

    DETECTION_PROMPT = """Analyze the following text and determine if it was written by AI or a human.

Analyze these indicators:
1. Sentence structure variation (humans have more varied patterns)
2. Word choice (AI tends to use formal, academic vocabulary)
3. Logical flow (AI is often too structured and predictable)
4. Idioms and colloquialisms (humans use more informal expressions)
5. Personal opinions and emotional expressions (AI is often neutral)

Text to analyze:
---
{text}
---

Respond in JSON format ONLY:
{{
  "ai_probability": 0.0-1.0,
  "human_probability": 0.0-1.0,
  "classification": "ai" | "human" | "mixed",
  "confidence": 0.0-1.0,
  "reasoning": "brief explanation",
  "ai_indicators": ["indicator1", "indicator2"],
  "human_indicators": ["indicator1", "indicator2"]
}}"""

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        if not self.config.custom_llm_endpoint:
            return self._create_error_result(file_path, "CUSTOM_LLM_ENDPOINT not configured")

        if not HAS_REQUESTS:
            return self._create_error_result(file_path, "requests library not installed")

        try:
            # æˆªå–å‰5000å­—ç¬¦ç”¨äºåˆ†æ
            sample_text = text[:5000]
            prompt = self.DETECTION_PROMPT.format(text=sample_text)

            response = requests.post(
                f"{self.config.custom_llm_endpoint}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.custom_llm_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.config.custom_llm_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 500
                },
                timeout=60
            )
            response.raise_for_status()
            data = response.json()

            # è§£æLLMå“åº”
            content = data["choices"][0]["message"]["content"]

            # æå–JSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', content)
            if not json_match:
                return self._create_error_result(file_path, "Failed to parse LLM response as JSON")

            result = json.loads(json_match.group())

            return DetectionResult(
                file_path=file_path,
                service=f"CustomLLM ({self.config.custom_llm_model})",
                timestamp=datetime.now().isoformat(),
                ai_probability=result.get("ai_probability", 0.5),
                human_probability=result.get("human_probability", 0.5),
                mixed_probability=0.0,
                classification=result.get("classification", "uncertain"),
                confidence=result.get("confidence", 0.5),
                highlighted_ai_sentences=result.get("ai_indicators", []),
                text_length=len(text)
            )

        except Exception as e:
            return self._create_error_result(file_path, str(e))


class FallbackDetector(BaseDetector):
    """æœ¬åœ°è§„åˆ™æ£€æµ‹å™¨ (æ— éœ€APIï¼Œä½œä¸ºé™çº§æ–¹æ¡ˆ)"""

    # AIå…¸å‹è¯æœ¯ (æ¥è‡ªcheck_ai_traces.sh)
    AI_PHRASES_CN = [
        "è®©æˆ‘ä»¬æ·±å…¥", "è®©æˆ‘ä»¬æ¥çœ‹", "è®©æˆ‘ä»¬æ¢è®¨",
        "å€¼å¾—æ³¨æ„çš„æ˜¯", "å€¼å¾—ä¸€æ",
        "ç»¼ä¸Šæ‰€è¿°", "æ€»çš„æ¥è¯´", "æ€»è€Œè¨€ä¹‹",
        "ä¸å¯å¦è®¤", "æ¯«æ— ç–‘é—®", "æ˜¾è€Œæ˜“è§",
        "åœ¨å½“ä»Š.*æ—¶ä»£", "åœ¨.*çš„èƒŒæ™¯ä¸‹",
        "æˆ‘ä»¬å¯ä»¥å¾—å‡º", "å¯ä»¥å¾—å‡ºç»“è®º",
        "è¿™æ„å‘³ç€", "è¿™è¡¨æ˜", "è¿™è¯´æ˜"
    ]

    AI_PHRASES_EN = [
        "let's dive into", "let's explore", "let's take a look",
        "it's worth noting", "it is worth mentioning", "notably",
        "in conclusion", "to summarize", "to sum up", "all in all",
        "undeniably", "undoubtedly", "without a doubt",
        "in today's world", "in the current landscape",
        "we can conclude", "it can be concluded",
        "it's important to note", "it is essential to"
    ]

    def detect(self, text: str, file_path: str = "") -> DetectionResult:
        import re

        # æ£€æµ‹è¯­è¨€
        cn_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        en_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        lang = "cn" if cn_chars > en_words else "en"

        phrases = self.AI_PHRASES_CN if lang == "cn" else self.AI_PHRASES_EN

        # ç»Ÿè®¡AIè¯æœ¯
        ai_phrase_count = 0
        matched_phrases = []
        for phrase in phrases:
            matches = re.findall(phrase, text, re.IGNORECASE)
            if matches:
                ai_phrase_count += len(matches)
                matched_phrases.extend(matches)

        # è®¡ç®—Burstiness (å¥å­é•¿åº¦å˜åŒ–)
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

        if len(sentences) > 1:
            lengths = [len(s) for s in sentences]
            avg = sum(lengths) / len(lengths)
            variance = sum((l - avg) ** 2 for l in lengths) / len(lengths)
            stddev = variance ** 0.5
            burstiness = stddev / avg if avg > 0 else 0
        else:
            burstiness = 0.5

        # ç»¼åˆè¯„åˆ†
        # AIè¯æœ¯æƒ©ç½š: æ¯ä¸ªè¯æœ¯å¢åŠ 0.05 AIæ¦‚ç‡
        # Burstinesså¥–åŠ±: é«˜å˜åŒ–å‡å°‘AIæ¦‚ç‡
        base_ai_prob = 0.5
        ai_prob = base_ai_prob + (ai_phrase_count * 0.05) - (burstiness * 0.2)
        ai_prob = max(0.0, min(1.0, ai_prob))

        if ai_prob > self.config.ai_threshold:
            classification = "ai"
        elif ai_prob < self.config.human_threshold:
            classification = "human"
        else:
            classification = "mixed"

        return DetectionResult(
            file_path=file_path,
            service="FallbackDetector (rule-based)",
            timestamp=datetime.now().isoformat(),
            ai_probability=ai_prob,
            human_probability=1 - ai_prob,
            mixed_probability=0.0,
            classification=classification,
            confidence=0.6,  # è§„åˆ™æ£€æµ‹ç½®ä¿¡åº¦è¾ƒä½
            highlighted_ai_sentences=matched_phrases[:10],
            text_length=len(text),
            language=lang
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»æ£€æµ‹å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DeepAIDetector:
    """æ·±åº¦AIæ£€æµ‹å™¨ä¸»ç±»"""

    def __init__(self, config: Optional[DetectorConfig] = None):
        self.config = config or DetectorConfig.from_env()
        self._init_detector()

    # æ£€æµ‹å™¨æƒé‡ (åŸºäºå‡†ç¡®æ€§å’Œå¯é æ€§)
    DETECTOR_WEIGHTS = {
        "GPTZero": 0.25,
        "Originality.ai": 0.30,  # æœ€é«˜å‡†ç¡®æ€§
        "ZeroGPT": 0.15,
        "Copyleaks": 0.20,
        "CustomLLM": 0.10,
        "FallbackDetector (rule-based)": 0.05
    }

    def _init_detector(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.detectors = {
            "gptzero": GPTZeroDetector,
            "originality": OriginalityDetector,
            "zerogpt": ZeroGPTDetector,
            "copyleaks": CopyleaksDetector,
            "custom_llm": CustomLLMDetector,
            "fallback": FallbackDetector
        }

        detector_class = self.detectors.get(self.config.service, FallbackDetector)
        self.detector = detector_class(self.config)
        self.fallback = FallbackDetector(self.config)

    def detect_file(self, file_path: str) -> DetectionResult:
        """æ£€æµ‹å•ä¸ªæ–‡ä»¶"""
        path = Path(file_path)

        if not path.exists():
            return DetectionResult(
                file_path=file_path,
                service="DeepAIDetector",
                timestamp=datetime.now().isoformat(),
                ai_probability=0.5,
                human_probability=0.5,
                mixed_probability=0.0,
                classification="uncertain",
                confidence=0.0,
                error=f"File not found: {file_path}"
            )

        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            content = path.read_text(encoding="utf-8")
        except Exception as e:
            return DetectionResult(
                file_path=file_path,
                service="DeepAIDetector",
                timestamp=datetime.now().isoformat(),
                ai_probability=0.5,
                human_probability=0.5,
                mixed_probability=0.0,
                classification="uncertain",
                confidence=0.0,
                error=f"Failed to read file: {e}"
            )

        # ç§»é™¤Markdownå…ƒæ•°æ® (frontmatter)
        import re
        content = re.sub(r'^---[\s\S]*?---\n', '', content)

        # å°è¯•ä¸»æ£€æµ‹å™¨ï¼Œå¤±è´¥æ—¶ä½¿ç”¨é™çº§æ–¹æ¡ˆ
        result = self.detector.detect(content, file_path)

        if result.error and self.config.service != "fallback":
            print(f"âš ï¸ Primary detector failed: {result.error}")
            print("   Falling back to rule-based detection...")
            result = self.fallback.detect(content, file_path)

        return result

    def detect_directory(self, dir_path: str, pattern: str = "*.md") -> List[DetectionResult]:
        """æ£€æµ‹ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶"""
        path = Path(dir_path)
        results = []

        for file in sorted(path.glob(pattern)):
            print(f"ğŸ“„ Analyzing: {file.name}...")
            result = self.detect_file(str(file))
            results.append(result)

        return results

    def aggregate_detect(self, file_path: str, services: Optional[List[str]] = None) -> AggregateResult:
        """
        å¤šæ£€æµ‹å™¨èšåˆæ£€æµ‹ (v2.0æ–°å¢)

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            services: è¦ä½¿ç”¨çš„æ£€æµ‹æœåŠ¡åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰å·²é…ç½®çš„

        Returns:
            AggregateResult: èšåˆæ£€æµ‹ç»“æœ
        """
        path = Path(file_path)
        timestamp = datetime.now().isoformat()

        # è¯»å–æ–‡ä»¶
        if not path.exists():
            return AggregateResult(
                file_path=file_path,
                timestamp=timestamp,
                weighted_ai_probability=0.5,
                weighted_human_probability=0.5,
                consensus_classification="uncertain",
                consensus_confidence=0.0,
                agreement_ratio=0.0,
                recommendation="æ–‡ä»¶ä¸å­˜åœ¨",
                needs_review=True
            )

        try:
            content = path.read_text(encoding="utf-8")
            import re
            content = re.sub(r'^---[\s\S]*?---\n', '', content)
        except Exception as e:
            return AggregateResult(
                file_path=file_path,
                timestamp=timestamp,
                weighted_ai_probability=0.5,
                weighted_human_probability=0.5,
                consensus_classification="uncertain",
                consensus_confidence=0.0,
                agreement_ratio=0.0,
                recommendation=f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}",
                needs_review=True
            )

        # ç¡®å®šè¦ä½¿ç”¨çš„æ£€æµ‹å™¨
        if services is None:
            services = ["gptzero", "zerogpt", "originality", "copyleaks", "fallback"]

        # è¿è¡Œæ‰€æœ‰æ£€æµ‹å™¨
        results = []
        for service_name in services:
            detector_class = self.detectors.get(service_name)
            if not detector_class:
                continue

            detector = detector_class(self.config)
            print(f"  ğŸ” Running {service_name}...")
            result = detector.detect(content, file_path)
            results.append(result)

        # åˆ†ç¦»æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
        successful = [r for r in results if not r.error]
        failed = [r for r in results if r.error]

        if not successful:
            return AggregateResult(
                file_path=file_path,
                timestamp=timestamp,
                weighted_ai_probability=0.5,
                weighted_human_probability=0.5,
                consensus_classification="uncertain",
                consensus_confidence=0.0,
                agreement_ratio=0.0,
                individual_results=[r.to_dict() for r in results],
                successful_detectors=0,
                failed_detectors=len(failed),
                recommendation="æ‰€æœ‰æ£€æµ‹å™¨éƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®",
                needs_review=True
            )

        # è®¡ç®—åŠ æƒå¹³å‡
        total_weight = 0.0
        weighted_ai_sum = 0.0
        classifications = []

        for result in successful:
            weight = self.DETECTOR_WEIGHTS.get(result.service, 0.1)
            weighted_ai_sum += result.ai_probability * weight
            total_weight += weight
            classifications.append(result.classification)

        weighted_ai_prob = weighted_ai_sum / total_weight if total_weight > 0 else 0.5
        weighted_human_prob = 1 - weighted_ai_prob

        # å¤šæ•°æŠ•ç¥¨ç¡®å®šåˆ†ç±»
        from collections import Counter
        classification_counts = Counter(classifications)
        consensus_classification = classification_counts.most_common(1)[0][0]
        consensus_count = classification_counts[consensus_classification]

        # è®¡ç®—ä¸€è‡´æ€§
        agreement_ratio = consensus_count / len(successful) if successful else 0.0

        # è®¡ç®—ç½®ä¿¡åº¦ (åŸºäºä¸€è‡´æ€§å’ŒåŠ æƒæ¦‚ç‡)
        if consensus_classification == "ai":
            base_confidence = weighted_ai_prob
        elif consensus_classification == "human":
            base_confidence = weighted_human_prob
        else:
            base_confidence = 1 - abs(weighted_ai_prob - 0.5) * 2

        consensus_confidence = base_confidence * agreement_ratio

        # ç”Ÿæˆæ¨è
        if agreement_ratio >= 0.8:
            if consensus_classification == "ai":
                recommendation = "é«˜åº¦ä¸€è‡´åˆ¤å®šä¸ºAIç”Ÿæˆï¼Œå»ºè®®è¿›è¡Œäººæ€§åŒ–æ”¹å†™"
            elif consensus_classification == "human":
                recommendation = "é«˜åº¦ä¸€è‡´åˆ¤å®šä¸ºäººç±»å†™ä½œï¼Œå¯ä»¥å‘å¸ƒ"
            else:
                recommendation = "åˆ¤å®šä¸ºæ··åˆå†…å®¹ï¼Œå»ºè®®å®¡æ ¸åå‘å¸ƒ"
            needs_review = False
        elif agreement_ratio >= 0.6:
            recommendation = "æ£€æµ‹å™¨æ„è§æœ‰åˆ†æ­§ï¼Œå»ºè®®äººå·¥å®¡æ ¸"
            needs_review = True
        else:
            recommendation = "æ£€æµ‹å™¨ä¸¥é‡åˆ†æ­§ï¼Œå¼ºçƒˆå»ºè®®äººå·¥å®¡æ ¸"
            needs_review = True

        return AggregateResult(
            file_path=file_path,
            timestamp=timestamp,
            weighted_ai_probability=weighted_ai_prob,
            weighted_human_probability=weighted_human_prob,
            consensus_classification=consensus_classification,
            consensus_confidence=consensus_confidence,
            agreement_ratio=agreement_ratio,
            individual_results=[r.to_dict() for r in results],
            successful_detectors=len(successful),
            failed_detectors=len(failed),
            recommendation=recommendation,
            needs_review=needs_review
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_result(result: DetectionResult, json_output: bool = False):
    """æ‰“å°æ£€æµ‹ç»“æœ"""
    if json_output:
        print(result.to_json())
        return

    # é¢œè‰²å®šä¹‰
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    CYAN = '\033[0;36m'
    NC = '\033[0m'

    print(f"\n{BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{NC}")
    print(f"{BLUE}â•‘           æ·±åº¦å­¦ä¹ AIæ£€æµ‹æŠ¥å‘Š                               â•‘{NC}")
    print(f"{BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}")
    print(f"\n{CYAN}æ–‡ä»¶: {result.file_path}{NC}")
    print(f"{CYAN}æœåŠ¡: {result.service}{NC}")
    print(f"{CYAN}æ—¶é—´: {result.timestamp}{NC}")

    print(f"\n{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")
    print(f"{YELLOW}æ£€æµ‹ç»“æœ{NC}")
    print(f"{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")

    # åˆ†ç±»ç»“æœ
    if result.classification == "ai":
        color = RED
        icon = "âŒ"
        label = "AIç”Ÿæˆ"
    elif result.classification == "human":
        color = GREEN
        icon = "âœ…"
        label = "äººç±»å†™ä½œ"
    else:
        color = YELLOW
        icon = "âš ï¸"
        label = "æ··åˆ/ä¸ç¡®å®š"

    print(f"\n   åˆ†ç±»ç»“æœ: {color}{icon} {label}{NC}")
    print(f"   ç½®ä¿¡åº¦:   {result.confidence:.1%}")
    print(f"\n   AIæ¦‚ç‡:   {result.ai_probability:.1%}")
    print(f"   äººç±»æ¦‚ç‡: {result.human_probability:.1%}")

    if result.highlighted_ai_sentences:
        print(f"\n   {RED}æ£€æµ‹åˆ°çš„AIæŒ‡æ ‡:{NC}")
        for indicator in result.highlighted_ai_sentences[:5]:
            print(f"   - {indicator}")

    if result.error:
        print(f"\n   {RED}é”™è¯¯: {result.error}{NC}")

    print()


def print_aggregate_result(result: AggregateResult, json_output: bool = False):
    """æ‰“å°èšåˆæ£€æµ‹ç»“æœ (v2.0æ–°å¢)"""
    if json_output:
        print(result.to_json())
        return

    # é¢œè‰²å®šä¹‰
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    CYAN = '\033[0;36m'
    NC = '\033[0m'

    print(f"\n{BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{NC}")
    print(f"{BLUE}â•‘           å¤šæ£€æµ‹å™¨èšåˆåˆ†ææŠ¥å‘Š (v2.0)                       â•‘{NC}")
    print(f"{BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{NC}")
    print(f"\n{CYAN}æ–‡ä»¶: {result.file_path}{NC}")
    print(f"{CYAN}æ—¶é—´: {result.timestamp}{NC}")

    print(f"\n{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")
    print(f"{YELLOW}èšåˆç»“æœ{NC}")
    print(f"{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")

    # åˆ†ç±»ç»“æœ
    if result.consensus_classification == "ai":
        color = RED
        icon = "âŒ"
        label = "AIç”Ÿæˆ"
    elif result.consensus_classification == "human":
        color = GREEN
        icon = "âœ…"
        label = "äººç±»å†™ä½œ"
    else:
        color = YELLOW
        icon = "âš ï¸"
        label = "æ··åˆ/ä¸ç¡®å®š"

    print(f"\n   å…±è¯†åˆ†ç±»: {color}{icon} {label}{NC}")
    print(f"   å…±è¯†ç½®ä¿¡åº¦: {result.consensus_confidence:.1%}")
    print(f"   æ£€æµ‹å™¨ä¸€è‡´æ€§: {result.agreement_ratio:.1%}")
    print(f"\n   åŠ æƒAIæ¦‚ç‡: {result.weighted_ai_probability:.1%}")
    print(f"   åŠ æƒäººç±»æ¦‚ç‡: {result.weighted_human_probability:.1%}")

    print(f"\n   æˆåŠŸæ£€æµ‹å™¨: {result.successful_detectors}")
    print(f"   å¤±è´¥æ£€æµ‹å™¨: {result.failed_detectors}")

    # å„æ£€æµ‹å™¨è¯¦æƒ…
    if result.individual_results:
        print(f"\n{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")
        print(f"{YELLOW}å„æ£€æµ‹å™¨ç»“æœ{NC}")
        print(f"{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")

        for r in result.individual_results:
            service = r.get("service", "Unknown")
            ai_prob = r.get("ai_probability", 0.5)
            classification = r.get("classification", "unknown")
            error = r.get("error")

            if error:
                print(f"   {RED}âœ— {service}: å¤±è´¥ - {error[:50]}{NC}")
            else:
                if classification == "ai":
                    c = RED
                elif classification == "human":
                    c = GREEN
                else:
                    c = YELLOW
                print(f"   {c}â€¢ {service}: {classification} ({ai_prob:.1%} AI){NC}")

    # æ¨è
    print(f"\n{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")
    print(f"{YELLOW}å»ºè®®{NC}")
    print(f"{YELLOW}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{NC}")
    print(f"\n   {result.recommendation}")
    if result.needs_review:
        print(f"   {RED}âš ï¸ éœ€è¦äººå·¥å®¡æ ¸{NC}")

    print()


def main():
    parser = argparse.ArgumentParser(
        description="æ·±åº¦å­¦ä¹ AIæ£€æµ‹å·¥å…· v2.0 - æ”¯æŒå¤šæ£€æµ‹å™¨èšåˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•æ£€æµ‹å™¨æ¨¡å¼
  python deep_ai_detector.py --file article.md
  python deep_ai_detector.py --file article.md --service gptzero

  # å¤šæ£€æµ‹å™¨èšåˆæ¨¡å¼ (v2.0æ–°å¢)
  python deep_ai_detector.py --file article.md --aggregate
  python deep_ai_detector.py --file article.md --aggregate --json

  # æ‰¹é‡æ£€æµ‹
  python deep_ai_detector.py --dir 2026_01/ --pattern "*.md"

ç¯å¢ƒå˜é‡é…ç½®:
  GPTZERO_API_KEY        GPTZero APIå¯†é’¥
  ZEROGPT_API_KEY        ZeroGPT APIå¯†é’¥ (v2.0æ–°å¢)
  ORIGINALITY_API_KEY    Originality.ai APIå¯†é’¥
  COPYLEAKS_API_KEY      Copyleaks APIå¯†é’¥ (v2.0æ–°å¢)
  COPYLEAKS_EMAIL        Copyleaks é‚®ç®± (v2.0æ–°å¢)
  AI_DETECTOR_SERVICE    é»˜è®¤æ£€æµ‹æœåŠ¡ (gptzero/originality/zerogpt/copyleaks/custom_llm/fallback)
  AI_DETECTOR_THRESHOLD  AIåˆ¤å®šé˜ˆå€¼ (é»˜è®¤0.7)
"""
    )

    parser.add_argument("--file", "-f", help="è¦æ£€æµ‹çš„æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dir", "-d", help="è¦æ£€æµ‹çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--pattern", "-p", default="*.md", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.md)")
    parser.add_argument("--json", "-j", action="store_true", help="è¾“å‡ºJSONæ ¼å¼")
    parser.add_argument("--aggregate", "-a", action="store_true",
                       help="ä½¿ç”¨å¤šæ£€æµ‹å™¨èšåˆæ¨¡å¼ (v2.0æ–°å¢)")
    parser.add_argument("--service", "-s",
                       choices=["gptzero", "originality", "zerogpt", "copyleaks", "custom_llm", "fallback"],
                       help="æŒ‡å®šæ£€æµ‹æœåŠ¡")

    args = parser.parse_args()

    if not args.file and not args.dir:
        parser.print_help()
        sys.exit(1)

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    config = DetectorConfig.from_env()
    if args.service:
        config.service = args.service

    detector = DeepAIDetector(config)

    if args.file:
        if args.aggregate:
            # å¤šæ£€æµ‹å™¨èšåˆæ¨¡å¼
            print(f"ğŸ”„ ä½¿ç”¨å¤šæ£€æµ‹å™¨èšåˆæ¨¡å¼åˆ†æ: {args.file}")
            result = detector.aggregate_detect(args.file)
            print_aggregate_result(result, args.json)
        else:
            # å•æ£€æµ‹å™¨æ¨¡å¼
            result = detector.detect_file(args.file)
            print_result(result, args.json)

    elif args.dir:
        results = detector.detect_directory(args.dir, args.pattern)

        if args.json:
            print(json.dumps([r.to_dict() for r in results], ensure_ascii=False, indent=2))
        else:
            for result in results:
                print_result(result, False)

            # æ±‡æ€»
            ai_count = sum(1 for r in results if r.classification == "ai")
            human_count = sum(1 for r in results if r.classification == "human")
            mixed_count = sum(1 for r in results if r.classification in ("mixed", "uncertain"))

            print(f"\n{'='*60}")
            print(f"æ±‡æ€»: AI={ai_count}, äººç±»={human_count}, æ··åˆ/ä¸ç¡®å®š={mixed_count}")
            print(f"{'='*60}")


if __name__ == "__main__":
    main()
