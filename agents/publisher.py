#!/usr/bin/env python3
"""
Publisher Agent

è´Ÿè´£å¤šå¹³å°å‘å¸ƒã€æ ¼å¼é€‚é…å’ŒSEOä¼˜åŒ–ã€‚
"""

from typing import Optional, List, Any, Dict
from pathlib import Path
from datetime import datetime
import re
from .config import AGENT_ROLES, AgentConfig


def create_publisher_agent(
    config: Optional[AgentConfig] = None,
    tools: Optional[List[Any]] = None,
    **kwargs
):
    """
    åˆ›å»ºPublisher Agent

    Args:
        config: Agenté…ç½®
        tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        **kwargs: é¢å¤–å‚æ•°è¦†ç›–é»˜è®¤é…ç½®

    Returns:
        CrewAI Agentå®ä¾‹
    """
    try:
        from crewai import Agent
    except ImportError:
        from .researcher import MockAgent
        return MockAgent('publisher', AGENT_ROLES['publisher'])

    config = config or AgentConfig()
    role_config = AGENT_ROLES['publisher']

    agent_params = {
        'role': role_config['role'],
        'goal': role_config['goal'],
        'backstory': role_config['backstory'],
        'verbose': config.verbose,
        'allow_delegation': kwargs.get('allow_delegation', False),
        'max_iter': config.max_iter,
        'max_retry_limit': config.max_retry_limit,
    }

    if config.llm_model:
        agent_params['llm'] = config.llm_model

    if tools:
        agent_params['tools'] = tools

    agent_params.update(kwargs)

    return Agent(**agent_params)


# å¹³å°é…ç½®
PLATFORM_CONFIGS = {
    'github': {
        'name': 'GitHub',
        'format': 'markdown',
        'max_length': None,
        'features': ['code_blocks', 'tables', 'images', 'anchors'],
        'naming_convention': '{title}_EN.md / {title}_CN.md',
        'frontmatter': False,
        'canonical_method': 'readme_link',
    },
    'medium': {
        'name': 'Medium',
        'format': 'markdown',
        'max_length': 15000,  # çº¦15åˆ†é’Ÿé˜…è¯»
        'features': ['images', 'embeds', 'quotes', 'code_blocks'],
        'naming_convention': 'N/A (web editor)',
        'frontmatter': False,
        'canonical_method': 'api',
        'api_field': 'canonicalUrl',
    },
    'devto': {
        'name': 'Dev.to',
        'format': 'markdown',
        'max_length': None,
        'features': ['code_blocks', 'liquid_tags', 'embeds'],
        'naming_convention': 'article-title.md',
        'frontmatter': True,
        'frontmatter_fields': ['title', 'published', 'tags', 'canonical_url', 'cover_image'],
        'canonical_method': 'frontmatter',
    },
    'linkedin': {
        'name': 'LinkedIn',
        'format': 'rich_text',
        'max_length': 3000,  # æ–‡ç« æ¨¡å¼
        'features': ['images', 'mentions', 'hashtags'],
        'naming_convention': 'N/A (web editor)',
        'frontmatter': False,
        'canonical_method': 'footer_link',
    },
    'twitter': {
        'name': 'Twitter/X',
        'format': 'threads',
        'max_length': 280,  # å•æ¡
        'thread_max': 25,
        'features': ['images', 'links', 'hashtags', 'mentions'],
        'naming_convention': 'N/A (API/web)',
        'frontmatter': False,
        'canonical_method': 'link_card',
    },
    'wechat': {
        'name': 'å¾®ä¿¡å…¬ä¼—å·',
        'format': 'rich_text_html',
        'max_length': 20000,  # å­—ç¬¦é™åˆ¶
        'features': ['images', 'quotes', 'code_blocks', 'tables', 'cards'],
        'naming_convention': 'wechat_{slug}.md',
        'frontmatter': True,
        'frontmatter_fields': ['title', 'author', 'cover_image', 'summary', 'original'],
        'canonical_method': 'original_link',
        'image_specs': {
            'cover': {'width': 900, 'height': 383, 'ratio': '2.35:1'},
            'thumb': {'width': 200, 'height': 200, 'ratio': '1:1'},
            'content': {'max_width': 900},
        },
        'style_guide': {
            'font_size': 16,
            'line_height': 1.75,
            'paragraph_spacing': 15,
            'heading_sizes': {'h1': 22, 'h2': 20, 'h3': 18},
            'accent_color': '#1890ff',
        },
    },
    'zhihu': {
        'name': 'çŸ¥ä¹',
        'format': 'markdown',
        'max_length': 50000,
        'features': ['images', 'formulas', 'tables', 'code_blocks', 'citations'],
        'naming_convention': 'zhihu_{slug}.md',
        'frontmatter': False,
        'canonical_method': 'footer_link',
    },
    'xiaohongshu': {
        'name': 'å°çº¢ä¹¦',
        'format': 'rich_text',
        'max_length': 1000,  # ç¬”è®°æ­£æ–‡
        'features': ['images', 'emojis', 'hashtags', 'mentions'],
        'naming_convention': 'xhs_{slug}.md',
        'frontmatter': False,
        'canonical_method': 'none',
        'image_specs': {
            'cover': {'ratio': '3:4', 'min_width': 1080},
            'content': {'max_count': 18},
        },
    },
}


class PlatformAdapter:
    """å¹³å°å†…å®¹é€‚é…å™¨"""

    def __init__(self, primary_domain: str = "https://innora.ai/blog"):
        self.primary_domain = primary_domain
        self.platforms = PLATFORM_CONFIGS

    def adapt_for_platform(
        self,
        content: str,
        platform: str,
        metadata: Optional[Dict] = None
    ) -> str:
        """
        ä¸ºæŒ‡å®šå¹³å°é€‚é…å†…å®¹

        Args:
            content: åŸå§‹Markdownå†…å®¹
            platform: ç›®æ ‡å¹³å°
            metadata: æ–‡ç« å…ƒæ•°æ®

        Returns:
            é€‚é…åçš„å†…å®¹
        """
        if platform not in self.platforms:
            raise ValueError(f"Unknown platform: {platform}. "
                             f"Available: {list(self.platforms.keys())}")

        config = self.platforms[platform]
        metadata = metadata or {}

        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title = title_match.group(1) if title_match else metadata.get('title', 'Untitled')

        # ç”ŸæˆCanonical URL
        canonical_url = self._generate_canonical_url(title, metadata)

        # æ ¹æ®å¹³å°å¤„ç†
        if platform == 'github':
            return self._adapt_github(content, title, canonical_url)
        elif platform == 'medium':
            return self._adapt_medium(content, title, canonical_url)
        elif platform == 'devto':
            return self._adapt_devto(content, title, canonical_url, metadata)
        elif platform == 'linkedin':
            return self._adapt_linkedin(content, title, canonical_url)
        elif platform == 'twitter':
            return self._adapt_twitter(content, title, canonical_url)
        elif platform == 'wechat':
            return self._adapt_wechat(content, title, canonical_url, metadata)
        elif platform == 'zhihu':
            return self._adapt_zhihu(content, title, canonical_url, metadata)
        elif platform == 'xiaohongshu':
            return self._adapt_xiaohongshu(content, title, canonical_url, metadata)

        return content

    def _generate_canonical_url(self, title: str, metadata: Dict) -> str:
        """ç”ŸæˆCanonical URL"""
        if 'canonical_url' in metadata:
            return metadata['canonical_url']

        # ä»æ ‡é¢˜ç”Ÿæˆslug
        slug = title.lower()
        slug = re.sub(r'[^\w\s-]', '', slug)
        slug = re.sub(r'[\s_]+', '-', slug)
        slug = slug.strip('-')[:50]

        date = metadata.get('date', datetime.now().strftime('%Y/%m'))
        return f"{self.primary_domain}/{date}/{slug}"

    def _adapt_github(self, content: str, title: str, canonical_url: str) -> str:
        """é€‚é…GitHubæ ¼å¼"""
        # GitHub Markdownä¿æŒåŸæ ·ï¼Œæ·»åŠ é¡¶éƒ¨é“¾æ¥
        header = f"> ğŸ“„ Full article: [{title}]({canonical_url})\n\n"
        return header + content

    def _adapt_medium(self, content: str, title: str, canonical_url: str) -> str:
        """é€‚é…Mediumæ ¼å¼"""
        # Mediuméœ€è¦é€šè¿‡APIè®¾ç½®canonicalUrlï¼Œå†…å®¹ä¿æŒMarkdown
        # æ·»åŠ åº•éƒ¨ä¿¡æ¯
        footer = f"\n\n---\n\n*Originally published at [{self.primary_domain}]({canonical_url})*"
        return content + footer

    def _adapt_devto(
        self,
        content: str,
        title: str,
        canonical_url: str,
        metadata: Dict
    ) -> str:
        """é€‚é…Dev.toæ ¼å¼"""
        # ç”Ÿæˆfrontmatter
        tags = metadata.get('tags', ['security', 'cybersecurity'])
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(',')]
        tags = tags[:4]  # Dev.toæœ€å¤š4ä¸ªæ ‡ç­¾

        frontmatter = f"""---
title: "{title}"
published: true
tags: {', '.join(tags)}
canonical_url: {canonical_url}
---

"""
        # ç§»é™¤åŸæœ‰frontmatterå¦‚æœæœ‰
        content = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)

        return frontmatter + content

    def _adapt_linkedin(self, content: str, title: str, canonical_url: str) -> str:
        """é€‚é…LinkedInæ ¼å¼"""
        config = self.platforms['linkedin']
        max_length = config['max_length']

        # ç§»é™¤Markdownæ ¼å¼
        text = self._strip_markdown(content)

        # æˆªæ–­åˆ°é™åˆ¶é•¿åº¦
        if len(text) > max_length:
            text = text[:max_length - 100] + "...\n\n"
            text += f"ğŸ“– Read the full article: {canonical_url}"
        else:
            text += f"\n\n---\n\n*Originally published at: {canonical_url}*"

        # æ·»åŠ hashtags
        text += "\n\n#Cybersecurity #InfoSec #ThreatIntelligence"

        return text

    def _adapt_twitter(self, content: str, title: str, canonical_url: str) -> str:
        """é€‚é…Twitterçº¿ç¨‹æ ¼å¼"""
        # æå–å…³é”®ç‚¹ç”Ÿæˆçº¿ç¨‹
        sections = re.split(r'\n##\s+', content)
        threads = []

        # é¦–æ¡æ¨æ–‡
        first_tweet = f"ğŸ”’ {title}\n\nA thread ğŸ§µ\n\n{canonical_url}"
        threads.append(first_tweet)

        # ä»å„sectionæå–è¦ç‚¹
        for section in sections[1:6]:  # æœ€å¤š5ä¸ªsection
            lines = section.split('\n')
            section_title = lines[0].strip()
            # æå–ç¬¬ä¸€ä¸ªè¦ç‚¹
            for line in lines[1:]:
                line = line.strip()
                if line and not line.startswith('#'):
                    point = self._strip_markdown(line)[:250]
                    tweet = f"ğŸ“Œ {section_title}\n\n{point}"
                    threads.append(tweet)
                    break

        # ç»“å°¾æ¨æ–‡
        threads.append(f"ğŸ“– Full analysis here: {canonical_url}\n\nLike and retweet if you found this useful! ğŸ™")

        return "\n\n---\n\n".join(threads)

    def _strip_markdown(self, text: str) -> str:
        """ç§»é™¤Markdownæ ¼å¼"""
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
        # ç§»é™¤é“¾æ¥ï¼Œä¿ç•™æ–‡å­—
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
        # ç§»é™¤ç²—ä½“/æ–œä½“
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```[\s\S]*?```', '[code block]', text)
        text = re.sub(r'`([^`]+)`', r'\1', text)
        # ç§»é™¤å›¾ç‰‡
        text = re.sub(r'!\[([^\]]*)\]\([^)]+\)', r'[Image: \1]', text)

        return text.strip()

    def _adapt_wechat(
        self,
        content: str,
        title: str,
        canonical_url: str,
        metadata: Dict
    ) -> str:
        """
        é€‚é…å¾®ä¿¡å…¬ä¼—å·æ ¼å¼

        å¾®ä¿¡å…¬ä¼—å·ç‰¹ç‚¹ï¼š
        - æ”¯æŒå¯Œæ–‡æœ¬HTMLæ ¼å¼
        - å°é¢å›¾å°ºå¯¸ï¼š900Ã—383ï¼ˆ2.35:1ï¼‰
        - æ­£æ–‡å›¾ç‰‡æœ€å¤§å®½åº¦900px
        - è¡Œé—´è·1.75ï¼Œæ®µé—´è·15px
        - ä¸»è‰²#1890ff
        """
        config = self.platforms['wechat']

        # ç”Ÿæˆfrontmatter
        author = metadata.get('author', 'AIç ”ç©¶å‘˜')
        summary = metadata.get('summary', '')
        if not summary:
            # è‡ªåŠ¨æå–æ‘˜è¦ï¼ˆç¬¬ä¸€æ®µï¼‰
            first_para = re.search(r'^(?!#)(.+?)(?:\n\n|\n#)', content, re.DOTALL)
            summary = first_para.group(1).strip()[:120] if first_para else title

        frontmatter = f"""---
title: "{title}"
author: "{author}"
cover_image: "éœ€è¦ä¸Šä¼ 900Ã—383å°é¢å›¾"
summary: "{summary}"
original: true
canonical_url: "{canonical_url}"
platform: wechat
---

"""
        # ç§»é™¤åŸæœ‰frontmatter
        content = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)

        # è½¬æ¢Markdownä¸ºå¾®ä¿¡å‹å¥½æ ¼å¼
        adapted = self._markdown_to_wechat(content)

        # æ·»åŠ åº•éƒ¨ä¿¡æ¯
        footer = f"""

---

**åŸæ–‡é“¾æ¥**ï¼š{canonical_url}

**å…³äºä½œè€…**
{author}ï¼Œå…³æ³¨AIä¸ç§‘æŠ€åˆ›ä¸šã€‚æ¬¢è¿äº¤æµã€‚

"""
        return frontmatter + adapted + footer

    def _markdown_to_wechat(self, content: str) -> str:
        """
        å°†Markdownè½¬æ¢ä¸ºå¾®ä¿¡å…¬ä¼—å·å‹å¥½æ ¼å¼

        å¤„ç†è§„åˆ™ï¼š
        - ä¿ç•™å±‚çº§æ ‡é¢˜ç»“æ„
        - è½¬æ¢ä»£ç å—ä¸ºå¼•ç”¨æ ¼å¼
        - è¡¨æ ¼ä¿æŒMarkdownæ ¼å¼ï¼ˆå¾®ä¿¡æ”¯æŒï¼‰
        - æ·»åŠ è§†è§‰åˆ†éš”ç¬¦
        """
        # è½¬æ¢ç²—ä½“å¼ºè°ƒï¼ˆå¾®ä¿¡æ”¯æŒï¼‰
        result = content

        # è½¬æ¢å¼•ç”¨å—ï¼ˆæ·»åŠ å·¦è¾¹æ¡†æ ·å¼æç¤ºï¼‰
        result = re.sub(
            r'^>\s*(.+)$',
            r'> ğŸ’¬ \1',
            result,
            flags=re.MULTILINE
        )

        # è½¬æ¢ä»£ç å—ä¸ºå¼•ç”¨æ ¼å¼ï¼ˆå¾®ä¿¡ä»£ç æ˜¾ç¤ºæœ‰é™ï¼‰
        def code_to_quote(match):
            lang = match.group(1) or 'code'
            code = match.group(2).strip()
            # ä¿æŒä»£ç ä½†æ·»åŠ æ ‡è®°
            return f"\nğŸ“‹ **ä»£ç  ({lang})**\n```\n{code}\n```\n"

        result = re.sub(
            r'```(\w+)?\n([\s\S]*?)```',
            code_to_quote,
            result
        )

        # æ·»åŠ æ®µè½åˆ†éš”ï¼ˆå¾®ä¿¡éœ€è¦æ˜æ˜¾åˆ†éš”ï¼‰
        result = re.sub(r'\n\n', '\n\nã€€\n\n', result)

        # è½¬æ¢åˆ†éš”çº¿
        result = re.sub(r'^---$', '\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n', result, flags=re.MULTILINE)

        return result

    def _adapt_zhihu(
        self,
        content: str,
        title: str,
        canonical_url: str,
        metadata: Dict
    ) -> str:
        """
        é€‚é…çŸ¥ä¹æ ¼å¼

        çŸ¥ä¹ç‰¹ç‚¹ï¼š
        - æ”¯æŒå®Œæ•´Markdown
        - æ”¯æŒLaTeXå…¬å¼
        - æ”¯æŒå¼•ç”¨å’Œå‚è€ƒæ–‡çŒ®
        - ä¸“æ æ–‡ç« æ— å­—æ•°é™åˆ¶
        """
        # çŸ¥ä¹åŸºæœ¬ä¿æŒMarkdownæ ¼å¼
        # æ·»åŠ åº•éƒ¨ä¿¡æ¯
        footer = f"""

---

**æœ¬æ–‡é¦–å‘äº**ï¼š{canonical_url}

æ¬¢è¿å…³æ³¨æˆ‘çš„çŸ¥ä¹ä¸“æ ï¼Œè·å–æ›´å¤šAIä¸ç§‘æŠ€åˆ›ä¸šçš„æ·±åº¦åˆ†æã€‚

"""
        # æ·»åŠ å‚è€ƒæ–‡çŒ®æ ¼å¼åŒ–
        if 'æ¥æº' in content or 'Source' in content:
            footer += "\n**å‚è€ƒèµ„æ–™**å·²åœ¨æ–‡ä¸­æ ‡æ³¨ã€‚\n"

        return content + footer

    def _adapt_xiaohongshu(
        self,
        content: str,
        title: str,
        canonical_url: str,
        metadata: Dict
    ) -> str:
        """
        é€‚é…å°çº¢ä¹¦æ ¼å¼

        å°çº¢ä¹¦ç‰¹ç‚¹ï¼š
        - ç¬”è®°æ­£æ–‡é™åˆ¶1000å­—
        - éœ€è¦ç²¾ç‚¼æ ¸å¿ƒè§‚ç‚¹
        - å¤§é‡ä½¿ç”¨emoji
        - hashtagæ ¼å¼ #è¯é¢˜#
        - å›¾ç‰‡ä¸ºä¸»ï¼Œæ–‡å­—ä¸ºè¾…
        """
        config = self.platforms['xiaohongshu']
        max_length = config['max_length']

        # æå–æ ¸å¿ƒè§‚ç‚¹
        points = []

        # æå–æ‰€æœ‰åŠ ç²—æ–‡æœ¬ä½œä¸ºæ ¸å¿ƒè§‚ç‚¹
        bold_matches = re.findall(r'\*\*([^*]+)\*\*', content)
        points.extend(bold_matches[:5])

        # æå–åˆ—è¡¨é¡¹
        list_items = re.findall(r'^[-*]\s+(.+)$', content, re.MULTILINE)
        points.extend(list_items[:5])

        # ç”Ÿæˆå°çº¢ä¹¦é£æ ¼å†…å®¹
        emojis = ['ğŸ”¥', 'ğŸ’¡', 'âœ¨', 'ğŸ“Œ', 'ğŸ¯', 'ğŸ’ª', 'ğŸš€', 'â­']

        note = f"ã€{title}ã€‘\n\n"

        for i, point in enumerate(points[:6]):
            emoji = emojis[i % len(emojis)]
            clean_point = self._strip_markdown(point)[:100]
            note += f"{emoji} {clean_point}\n\n"

        # æ·»åŠ hashtags
        tags = metadata.get('tags', ['AIåˆ›ä¸š', 'ç§‘æŠ€è¶‹åŠ¿', 'è¶…çº§ä¸ªä½“'])
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(',')]

        note += "\n"
        for tag in tags[:10]:
            note += f"#{tag}# "

        # æˆªæ–­åˆ°é™åˆ¶é•¿åº¦
        if len(note) > max_length:
            note = note[:max_length - 50] + "...\n\næ›´å¤šå†…å®¹è§ä¸»é¡µï½"

        # æ·»åŠ å°é¢å›¾è¯´æ˜
        frontmatter = f"""---
platform: xiaohongshu
title: "{title}"
cover_ratio: "3:4"
cover_min_width: 1080
image_count: å»ºè®®9å¼ å›¾
---

"""
        return frontmatter + note


class PublishingChecklist:
    """å‘å¸ƒæ£€æŸ¥æ¸…å•"""

    CHECKLIST = {
        'pre_publish': [
            'AIæ£€æµ‹ç‡ < 15%',
            'å¯è¯»æ€§è¯„åˆ† > 60',
            'æ‹¼å†™å’Œè¯­æ³•æ£€æŸ¥é€šè¿‡',
            'æ‰€æœ‰é“¾æ¥å¯è®¿é—®',
            'å›¾ç‰‡æœ‰altæ–‡æœ¬',
            'Schema Markupå·²æ·»åŠ ',
        ],
        'seo': [
            'æ ‡é¢˜åŒ…å«å…³é”®è¯',
            'æè¿°åœ¨155å­—ç¬¦å†…',
            'æœ‰H2/H3å±‚çº§ç»“æ„',
            'å†…éƒ¨é“¾æ¥å·²æ·»åŠ ',
            'Canonical URLå·²è®¾ç½®',
        ],
        'platform_specific': {
            'github': ['READMEé“¾æ¥æ›´æ–°', 'ç›®å½•ç»“æ„æ­£ç¡®'],
            'medium': ['å°é¢å›¾ç‰‡å·²ä¸Šä¼ ', 'æ ‡ç­¾å·²æ·»åŠ ï¼ˆæœ€å¤š5ä¸ªï¼‰'],
            'devto': ['frontmatterå®Œæ•´', 'æ ‡ç­¾å·²æ·»åŠ ï¼ˆæœ€å¤š4ä¸ªï¼‰'],
            'linkedin': ['æ‘˜è¦ç‰ˆæœ¬å·²å‡†å¤‡', 'hashtagså·²æ·»åŠ '],
            'twitter': ['çº¿ç¨‹å·²åˆ†å‰²', 'å…³é”®ç‚¹å·²æç‚¼'],
            'wechat': [
                'å°é¢å›¾å·²ä¸Šä¼ ï¼ˆ900Ã—383ï¼‰',
                'æ­£æ–‡å›¾ç‰‡å®½åº¦â‰¤900px',
                'åŸåˆ›å£°æ˜å·²å‹¾é€‰',
                'èµèµåŠŸèƒ½å·²å¼€å¯',
                'é˜…è¯»åŸæ–‡é“¾æ¥å·²è®¾ç½®',
                'æ— æ•æ„Ÿè¯ï¼ˆå·²æ£€æµ‹ï¼‰',
                'AIå†…å®¹æ ‡è¯†å·²æ·»åŠ ',
            ],
            'zhihu': [
                'ä¸“æ å·²é€‰æ‹©',
                'è¯é¢˜æ ‡ç­¾å·²æ·»åŠ ',
                'å‚è€ƒæ–‡çŒ®æ ¼å¼æ­£ç¡®',
                'åŸåˆ›å£°æ˜å·²å‹¾é€‰',
            ],
            'xiaohongshu': [
                'å°é¢å›¾æ¯”ä¾‹3:4',
                'å›¾ç‰‡æ•°é‡â‰¤18å¼ ',
                'æ­£æ–‡â‰¤1000å­—',
                'hashtagæ ¼å¼æ­£ç¡®ï¼ˆ#è¯é¢˜#ï¼‰',
                'AIå†…å®¹å·²æ ‡è¯†',
            ],
        },
    }

    @classmethod
    def get_checklist(cls, platform: Optional[str] = None) -> Dict:
        """è·å–å‘å¸ƒæ£€æŸ¥æ¸…å•"""
        checklist = {
            'pre_publish': cls.CHECKLIST['pre_publish'],
            'seo': cls.CHECKLIST['seo'],
        }

        if platform and platform in cls.CHECKLIST['platform_specific']:
            checklist['platform_specific'] = cls.CHECKLIST['platform_specific'][platform]

        return checklist


# å‘å¸ƒæ—¶é—´å»ºè®®
OPTIMAL_POSTING_TIMES = {
    'github': {
        'best_days': ['Tuesday', 'Wednesday', 'Thursday'],
        'best_hours': ['10:00', '14:00'],
        'timezone': 'UTC',
    },
    'medium': {
        'best_days': ['Tuesday', 'Wednesday'],
        'best_hours': ['08:00', '11:00'],
        'timezone': 'EST',
    },
    'devto': {
        'best_days': ['Monday', 'Tuesday', 'Wednesday'],
        'best_hours': ['07:00', '12:00'],
        'timezone': 'UTC',
    },
    'linkedin': {
        'best_days': ['Tuesday', 'Wednesday', 'Thursday'],
        'best_hours': ['08:00', '10:00', '12:00'],
        'timezone': 'EST',
    },
    'twitter': {
        'best_days': ['Wednesday', 'Thursday'],
        'best_hours': ['09:00', '12:00', '17:00'],
        'timezone': 'EST',
    },
    'wechat': {
        'best_days': ['Tuesday', 'Wednesday', 'Thursday', 'Friday'],
        'best_hours': ['08:00', '12:00', '20:00', '22:00'],
        'timezone': 'Asia/Shanghai',
        'notes': 'æ—©8ç‚¹é€šå‹¤ã€åˆä¼‘ã€æ™šé—´é˜…è¯»é«˜å³°',
    },
    'zhihu': {
        'best_days': ['Monday', 'Tuesday', 'Wednesday', 'Thursday'],
        'best_hours': ['10:00', '14:00', '21:00'],
        'timezone': 'Asia/Shanghai',
        'notes': 'å·¥ä½œæ—¥çŸ¥è¯†æ¶ˆè´¹æ´»è·ƒ',
    },
    'xiaohongshu': {
        'best_days': ['Friday', 'Saturday', 'Sunday'],
        'best_hours': ['12:00', '18:00', '21:00', '22:00'],
        'timezone': 'Asia/Shanghai',
        'notes': 'å‘¨æœ«å¥³æ€§ç”¨æˆ·æ´»è·ƒåº¦é«˜',
    },
}


def get_optimal_posting_time(platform: str) -> Dict:
    """è·å–å¹³å°æœ€ä½³å‘å¸ƒæ—¶é—´"""
    if platform not in OPTIMAL_POSTING_TIMES:
        return {'message': f'No data for {platform}'}
    return OPTIMAL_POSTING_TIMES[platform]
